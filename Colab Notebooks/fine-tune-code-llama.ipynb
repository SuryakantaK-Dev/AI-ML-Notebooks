{"cells":[{"cell_type":"markdown","metadata":{"id":"_5rfduRD1CSF"},"source":["# A guide to fine-tuning Code Llama\n","\n","**In this guide I show you how to fine-tune Code Llama to become a beast of an SQL developer. For coding tasks, you can generally get much better performance out of Code Llama than Llama 2, especially when you specialise the model on a particular task:**\n","\n","- I use the [b-mc2/sql-create-context](https://huggingface.co/datasets/b-mc2/sql-create-context) which is a bunch of text queries and their corresponding SQL queries\n","- A Lora approach, quantizing the base model to int 8, freezing its weights and only training an adapter\n","- Much of the code is borrowed from [alpaca-lora](https://github.com/tloen/alpaca-lora), but I refactored it quite a bit for this\n"]},{"cell_type":"markdown","metadata":{"id":"HFQ9XeBI1CSG"},"source":["### 2. Pip installs\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y0x4OWfq1CSG","outputId":"395a8e6e-e017-4add-9fa4-47e5f52eb8ef","executionInfo":{"status":"ok","timestamp":1726713843546,"user_tz":-330,"elapsed":90508,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/huggingface/transformers.git@main\n","  Cloning https://github.com/huggingface/transformers.git (to revision main) to /tmp/pip-req-build-jtj1kjhy\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-jtj1kjhy\n","  Resolved https://github.com/huggingface/transformers.git to commit 5af7d41e49bbfc8319f462eb45253dcb3863dfb7\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (3.16.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.45.0.dev0) (4.66.5)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.0+cu121)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.45.0.dev0) (2024.8.30)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n","Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: transformers\n","  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9761399 sha256=bd6468b63841c7d58e992f8f81ee2db70ee8d667f7a9df518924fc50d3f31d85\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-rt_qgh7e/wheels/cf/59/82/6492402e887a68975030bf8c06532260abc16abb7ccd8127cc\n","Successfully built transformers\n","Installing collected packages: bitsandbytes, transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.44.2\n","    Uninstalling transformers-4.44.2:\n","      Successfully uninstalled transformers-4.44.2\n","Successfully installed bitsandbytes-0.43.3 transformers-4.45.0.dev0\n","Collecting git+https://github.com/huggingface/peft.git@4c611f4\n","  Cloning https://github.com/huggingface/peft.git (to revision 4c611f4) to /tmp/pip-req-build-rwcju4vs\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-rwcju4vs\n","\u001b[33m  WARNING: Did not find branch or tag '4c611f4', assuming revision or ref.\u001b[0m\u001b[33m\n","\u001b[0m  Running command git checkout -q 4c611f4\n","  Resolved https://github.com/huggingface/peft.git to commit 4c611f4\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0.dev0) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0.dev0) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0.dev0) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0.dev0) (6.0.2)\n","Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0.dev0) (2.4.0+cu121)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0.dev0) (4.45.0.dev0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0.dev0) (4.66.5)\n","Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0.dev0) (0.34.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.6.0.dev0) (0.4.5)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->peft==0.6.0.dev0) (0.24.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (3.16.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.6.0.dev0) (2024.6.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.0.dev0) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.0.dev0) (2.32.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.6.0.dev0) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.6.0.dev0) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.0.dev0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.0.dev0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.0.dev0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.6.0.dev0) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.6.0.dev0) (1.3.0)\n","Building wheels for collected packages: peft\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for peft: filename=peft-0.6.0.dev0-py3-none-any.whl size=107773 sha256=c73bf2f6a79a53cd474079408f418a56826e981e462d9b748a085af233560974\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-zimxn3dd/wheels/09/7c/24/ec39b8742e1a5e1de8dab6e557914d7709f4d5797838ab104f\n","Successfully built peft\n","Installing collected packages: peft\n","Successfully installed peft-0.6.0.dev0\n","Collecting datasets==2.10.1\n","  Downloading datasets-2.10.1-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (1.26.4)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (14.0.2)\n","Collecting dill<0.3.7,>=0.3.0 (from datasets==2.10.1)\n","  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (2.1.4)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (4.66.5)\n","Collecting xxhash (from datasets==2.10.1)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets==2.10.1)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.11.1->datasets==2.10.1) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (3.10.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (0.24.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (24.1)\n","Collecting responses<0.19 (from datasets==2.10.1)\n","  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.10.1) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.10.1) (4.0.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (3.16.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.10.1) (2024.8.30)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets==2.10.1)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.10.1) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.10.1) (1.16.0)\n","Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, dill, responses, multiprocess, datasets\n","Successfully installed datasets-2.10.1 dill-0.3.6 multiprocess-0.70.14 responses-0.18.0 xxhash-3.5.0\n","Collecting wandb\n","  Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","Collecting docker-pycreds>=0.4.0 (from wandb)\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n","Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n","  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.3)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n","Collecting sentry-sdk>=1.0.0 (from wandb)\n","  Downloading sentry_sdk-2.14.0-py2.py3-none-any.whl.metadata (9.7 kB)\n","Collecting setproctitle (from wandb)\n","  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (71.0.4)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n","  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n","Downloading wandb-0.18.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentry_sdk-2.14.0-py2.py3-none-any.whl (311 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.4/311.4 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n","Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.14.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.1\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n","Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n"]}],"source":["!pip install git+https://github.com/huggingface/transformers.git@main bitsandbytes  # we need latest transformers for this\n","!pip install git+https://github.com/huggingface/peft.git@4c611f4\n","!pip install datasets==2.10.1\n","import locale # colab workaround\n","locale.getpreferredencoding = lambda: \"UTF-8\" # colab workaround\n","!pip install wandb\n","!pip install scipy"]},{"cell_type":"markdown","metadata":{"id":"mCmGzYg51CSH"},"source":["I used an A100 GPU machine with Python 3.10 and cuda 11.8 to run this notebook. It took about an hour to run."]},{"cell_type":"markdown","metadata":{"id":"0mu9JczX1CSH"},"source":["### Loading libraries\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"oTeYW8z51CSH","executionInfo":{"status":"ok","timestamp":1726714296931,"user_tz":-330,"elapsed":26584,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}}},"outputs":[],"source":["from datetime import datetime\n","import os\n","import sys\n","\n","import torch\n","from peft import (\n","    LoraConfig,\n","    get_peft_model,\n","    get_peft_model_state_dict,\n","    prepare_model_for_int8_training,\n","    set_peft_model_state_dict,\n",")\n","from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n"]},{"cell_type":"code","source":["!pip uninstall torch\n","!pip install torch\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_LQoprvKyoX","executionInfo":{"status":"ok","timestamp":1726714262857,"user_tz":-330,"elapsed":48025,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"1bcd7665-c15e-4d70-a7a2-74f90ba877b6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: torch 2.4.1\n","Uninstalling torch-2.4.1:\n","  Would remove:\n","    /usr/local/bin/convert-caffe2-to-onnx\n","    /usr/local/bin/convert-onnx-to-caffe2\n","    /usr/local/bin/torchrun\n","    /usr/local/lib/python3.10/dist-packages/functorch/*\n","    /usr/local/lib/python3.10/dist-packages/torch-2.4.1.dist-info/*\n","    /usr/local/lib/python3.10/dist-packages/torch/*\n","    /usr/local/lib/python3.10/dist-packages/torchgen/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled torch-2.4.1\n","Collecting torch\n","  Using cached torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n","Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.0.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.68)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Using cached torch-2.4.1-cp310-cp310-manylinux1_x86_64.whl (797.1 MB)\n","Installing collected packages: torch\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.4.0+cu121 requires torch==2.4.0, but you have torch 2.4.1 which is incompatible.\n","torchvision 0.19.0+cu121 requires torch==2.4.0, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-2.4.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"32zH9-hM1CSH"},"source":["(If you have import errors, try restarting your Jupyter kernel)\n"]},{"cell_type":"markdown","metadata":{"id":"4M9KyT0S1CSH"},"source":["### Load dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w44O1EK-1CSH"},"outputs":[],"source":["from datasets import load_dataset\n","dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n","train_dataset = dataset.train_test_split(test_size=0.1)[\"train\"]\n","eval_dataset = dataset.train_test_split(test_size=0.1)[\"test\"]"]},{"cell_type":"markdown","metadata":{"id":"fJ54EffO1CSI"},"source":["The above pulls the dataset from the Huggingface Hub and splits 10% of it into an evaluation set to check how well the model is doing through training. If you want to load your own dataset do this:\n","\n","```\n","train_dataset = load_dataset('json', data_files='train_set.jsonl', split='train')\n","eval_dataset = load_dataset('json', data_files='validation_set.jsonl', split='train')\n","```\n","\n","And if you want to view any samples in the dataset just do something like:``` ```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fFbeaZzf1CSJ"},"outputs":[],"source":["print(train_dataset[3])"]},{"cell_type":"markdown","metadata":{"id":"VHdMYcu61CSJ"},"source":["Each entry is made up of a text 'question', the sql table 'context' and the 'answer'."]},{"cell_type":"markdown","metadata":{"id":"6ig7NvWN1CSJ"},"source":["### Load model\n","I load code llama from huggingface in int8. Standard for Lora:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMnU93bY1CSJ"},"outputs":[],"source":["base_model = \"codellama/CodeLlama-7b-hf\"\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    load_in_8bit=True,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")"]},{"cell_type":"markdown","metadata":{"id":"F3OF-wtj1CSJ"},"source":["torch_dtype=torch.float16 means computations are performed using a float16 representation, even though the values themselves are 8 bit ints.\n","\n","If you get error \"ValueError: Tokenizer class CodeLlamaTokenizer does not exist or is not currently imported.\" Make sure you have transformers version is 4.33.0.dev0 and accelerate is >=0.20.3.\n"]},{"cell_type":"markdown","metadata":{"id":"_2VXqJJe1CSJ"},"source":["### 3. Check base model\n","A very good common practice is to check whether a model can already do the task at hand. Fine-tuning is something you want to try to avoid at all cost:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiyAff1a1CSJ"},"outputs":[],"source":["eval_prompt = \"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n","\n","You must output the SQL query that answers the question.\n","### Input:\n","Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n","\n","### Context:\n","CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n","\n","### Response:\n","\"\"\"\n","# {'question': 'Name the comptroller for office of prohibition', 'context': 'CREATE TABLE table_22607062_1 (comptroller VARCHAR, ticket___office VARCHAR)', 'answer': 'SELECT comptroller FROM table_22607062_1 WHERE ticket___office = \"Prohibition\"'}\n","model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","model.eval()\n","with torch.no_grad():\n","    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{"id":"544rmsdU1CSJ"},"source":["I get the output:\n","```\n","SELECT * FROM table_name_12 WHERE class > 91.5 AND city_of_license = 'hyannis, nebraska'\n","```\n","which is clearly wrong if the input is asking for just class!"]},{"cell_type":"markdown","metadata":{"id":"puuOXL2R1CSJ"},"source":["### 4. Tokenization\n","Setup some tokenization settings like left padding because it makes [training use less memory](https://ai.stackexchange.com/questions/41485/while-fine-tuning-a-decoder-only-llm-like-llama-on-chat-dataset-what-kind-of-pa):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1P0mphA1CSJ"},"outputs":[],"source":["tokenizer.add_eos_token = True\n","tokenizer.pad_token_id = 0\n","tokenizer.padding_side = \"left\""]},{"cell_type":"markdown","metadata":{"id":"kpo66hMo1CSJ"},"source":["Setup the tokenize function to make labels and input_ids the same. This is basically what [self-supervised fine-tuning](https://neptune.ai/blog/self-supervised-learning) is:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gjSerml71CSJ"},"outputs":[],"source":["def tokenize(prompt):\n","    result = tokenizer(\n","        prompt,\n","        truncation=True,\n","        max_length=512,\n","        padding=False,\n","        return_tensors=None,\n","    )\n","\n","    # \"self-supervised learning\" means the labels are also the inputs:\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"fNk_3THv1CSK"},"source":["And run convert each data_point into a prompt that I found online that works quite well:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MvO7A-ZF1CSK"},"outputs":[],"source":["def generate_and_tokenize_prompt(data_point):\n","    full_prompt =f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n","\n","You must output the SQL query that answers the question.\n","\n","### Input:\n","{data_point[\"question\"]}\n","\n","### Context:\n","{data_point[\"context\"]}\n","\n","### Response:\n","{data_point[\"answer\"]}\n","\"\"\"\n","    return tokenize(full_prompt)"]},{"cell_type":"markdown","metadata":{"id":"sLvwo-ax1CSK"},"source":["Reformat to prompt and tokenize each sample:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SA2BqSzW1CSK"},"outputs":[],"source":["tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n","tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"]},{"cell_type":"markdown","metadata":{"id":"Z5ByhitV1CSK"},"source":["### 5. Setup Lora"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9q_26pz71CSK"},"outputs":[],"source":["model.train() # put model back into training mode\n","model = prepare_model_for_int8_training(model)\n","\n","config = LoraConfig(\n","    r=16,\n","    lora_alpha=16,\n","    target_modules=[\n","    \"q_proj\",\n","    \"k_proj\",\n","    \"v_proj\",\n","    \"o_proj\",\n","],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n",")\n","model = get_peft_model(model, config)"]},{"cell_type":"markdown","metadata":{"id":"GW1wtvbP1CSK"},"source":["Optional stuff to setup Weights and Biases to view training graphs:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AC1_qWbS1CSK"},"outputs":[],"source":["wandb_project = \"sql-try2-coder\"\n","if len(wandb_project) > 0:\n","    os.environ[\"WANDB_PROJECT\"] = wandb_project\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5iv4txxu1CSK"},"outputs":[],"source":["if torch.cuda.device_count() > 1:\n","    # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n","    model.is_parallelizable = True\n","    model.model_parallel = True"]},{"cell_type":"markdown","metadata":{"id":"pYSsnciQ1CSK"},"source":["### 6. Training arguments\n","If you run out of GPU memory, change per_device_train_batch_size. The gradient_accumulation_steps variable should ensure this doesn't affect batch dynamics during the training run. All the other variables are standard stuff that I wouldn't recommend messing with:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uvEi1kP21CSK"},"outputs":[],"source":["batch_size = 128\n","per_device_train_batch_size = 32\n","gradient_accumulation_steps = batch_size // per_device_train_batch_size\n","output_dir = \"sql-code-llama\"\n","\n","training_args = TrainingArguments(\n","        per_device_train_batch_size=per_device_train_batch_size,\n","        gradient_accumulation_steps=gradient_accumulation_steps,\n","        warmup_steps=100,\n","        max_steps=400,\n","        learning_rate=3e-4,\n","        fp16=True,\n","        logging_steps=10,\n","        optim=\"adamw_torch\",\n","        evaluation_strategy=\"steps\", # if val_set_size > 0 else \"no\",\n","        save_strategy=\"steps\",\n","        eval_steps=20,\n","        save_steps=20,\n","        output_dir=output_dir,\n","        # save_total_limit=3,\n","        load_best_model_at_end=False,\n","        # ddp_find_unused_parameters=False if ddp else None,\n","        group_by_length=True, # group sequences of roughly the same length together to speed up training\n","        report_to=\"wandb\", # if use_wandb else \"none\",\n","        run_name=f\"codellama-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\", # if use_wandb else None,\n","    )\n","\n","trainer = Trainer(\n","    model=model,\n","    train_dataset=tokenized_train_dataset,\n","    eval_dataset=tokenized_val_dataset,\n","    args=training_args,\n","    data_collator=DataCollatorForSeq2Seq(\n","        tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n","    ),\n",")"]},{"cell_type":"markdown","metadata":{"id":"1aJp6Jxl1CSK"},"source":["Then we do some pytorch-related optimisation (which just make training faster but don't affect accuracy):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ycCHZZl1CSK"},"outputs":[],"source":["model.config.use_cache = False\n","\n","old_state_dict = model.state_dict\n","model.state_dict = (lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())).__get__(\n","    model, type(model)\n",")\n","if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n","    print(\"compiling the model\")\n","    model = torch.compile(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bF5oWKxK1CSL"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"j1dRQLMT1CSU"},"source":["### Load the final checkpoint\n","Now for the moment of truth! Has our work paid off...?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRdVgDTg1CSU"},"outputs":[],"source":["import torch\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n","\n","base_model = \"codellama/CodeLlama-7b-hf\"\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model,\n","    load_in_8bit=True,\n","    torch_dtype=torch.float16,\n","    device_map=\"auto\",\n",")\n","tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")"]},{"cell_type":"markdown","metadata":{"id":"76UlHzhy1CSU"},"source":["To load a fine-tuned Lora/Qlora adapter use PeftModel.from_pretrained. ```output_dir``` should be something containing an adapter_config.json and adapter_model.bin:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQrCR0os1CSU"},"outputs":[],"source":["from peft import PeftModel\n","model = PeftModel.from_pretrained(model, output_dir)"]},{"cell_type":"markdown","metadata":{"id":"roqy_WRi1CSU"},"source":["Try the same prompt as before:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrCxouNp1CSU"},"outputs":[],"source":["eval_prompt = \"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables.\n","\n","You must output the SQL query that answers the question.\n","### Input:\n","Which Class has a Frequency MHz larger than 91.5, and a City of license of hyannis, nebraska?\n","\n","### Context:\n","CREATE TABLE table_name_12 (class VARCHAR, frequency_mhz VARCHAR, city_of_license VARCHAR)\n","\n","### Response:\n","\"\"\"\n","\n","model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","model.eval()\n","with torch.no_grad():\n","    print(tokenizer.decode(model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))\n"]},{"cell_type":"markdown","metadata":{"id":"I6IcTOCq1CSU"},"source":["And the model outputs:\n","```\n","SELECT class FROM table_name_12 WHERE frequency_mhz > 91.5 AND city_of_license = \"hyannis, nebraska\"\n","```\n","So it works! If you want to convert your this adapter to a Llama.cpp model to run locally follow my other [guide](https://ragntune.com/blog/A-guide-to-running-Llama-2-qlora-loras-on-Llama.cpp). If you have any questions, shoot me a message on [Elon Musk's website](https://twitter.com/samlhuillier_).\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}