{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOt9dTZDUvXA1GWG4pYkwlH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Q1"],"metadata":{"id":"OvW8AKh64hIw"}},{"cell_type":"markdown","source":["\n","This is an article about a system called MADEx that detects medications, adverse drug events, and their relations in clinical notes. It discusses the challenges of using clinical text for this purpose, and the importance of accurate detection for pharmacovigilance. MADEx is a machine learning-based system that uses recurrent neural networks (RNNs). The system was evaluated in a competition and achieved high performance.\n","\n","#Literature Review on ADE Detection:\n","\n","The paper titled \"MADEx: A Machine Learning-based NLP System for Detecting Medications, Adverse Drug Events, and Their Relations from Clinical Notes\" discusses the use of deep learning models in the context of Adverse Drug Events (ADEs) detection from Electronic Health Records (EHRs). The key findings, methodologies, datasets, and performance metrics from the relevant studies are summarized as follows:\n","\n","#1. Key Findings:\n","\n","Early detection of ADEs from EHRs is crucial for pharmacovigilance and drug safety surveillance.\n","Clinical NLP plays a vital role in extracting information from unstructured clinical text for ADE detection.\n","The study introduces MADEx, a machine learning-based clinical NLP system, for detecting medications, ADEs, and their relations from clinical notes.\n","MADEx utilizes a Recurrent Neural Network (RNN) model with Long Short-Term Memory (LSTM) for clinical Named Entity Recognition (NER) and a Support Vector Machines (SVMs) model for relation extraction.\n","\n","#2. Methodologies:\n","\n","For clinical NER, the study compares the LSTM-CRFs model with a baseline Conditional Random Fields (CRFs) model. The LSTM-CRFs model incorporates character-level embedding, bidirectional LSTM, and dropout.\n","Relation extraction involves the comparison of SVMs and Random Forests (RFs) for both single-sentence and cross-sentence relations.\n","The integrated pipeline combines the NER and relation extraction modules into a unified system for extracting entities and relations together.\n","\n","#3. Datasets:\n","\n","The study uses the Medication and Adverse Drug Events (MADE1.0) challenge dataset, consisting of 1,089 de-identified clinical notes with annotations for medications, ADEs, indications, and other signs and symptoms.\n","The dataset is divided into a training set of 876 notes and a test set of 213 notes, with annotations for 79,114 entities and 27,175 relations.\n","\n","#4. Performance Metrics:\n","\n","MADEx achieves top three best performance (F1-score of 0.8233) for clinical NER in the 2018 MADE1.0 challenge.\n","The relation extraction module and integrated pipeline of MADEx are comparable to the best systems developed in the challenge according to post-challenge evaluation.\n","\n","#5. Conclusion:\n","\n","The study demonstrates the efficiency of deep learning methods, specifically LSTM-CRFs, for automatic extraction of medications, ADEs, and their relations from clinical text.\n","Combining recurrent neural networks and support vector machines in a hybrid system achieves good performance in detecting medications, adverse drug events, and their relations.\n","The absence of validation data highlights the importance of having more samples in training, emphasizing the significance of dataset size in the absence of validation sets.\n","\n","In summary, the paper highlights the advancements in ADE detection using deep learning models and emphasizes the significance of clinical NLP in extracting valuable information from unstructured clinical text for pharmacovigilance and drug safety surveillance. MADEx emerges as an effective system in this context, showcasing the potential of hybrid approaches combining deep learning and traditional machine learning methods."],"metadata":{"id":"s0W2i6eG4tvU"}},{"cell_type":"markdown","source":["#Q2"],"metadata":{"id":"Ma59bpuz9glq"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from tqdm import tqdm\n","\n","# Assuming the ADE dataset is loaded into two separate lists: sentences and labels\n","# You can modify the loading logic based on the actual format of your dataset\n","\n","# Example loading logic\n","# with open('DRUG-AE.rel', 'r') as file:\n","#     lines = file.readlines()\n","#     sentences = [line.split('|')[1].strip() for line in lines]\n","#     labels = [1 if 'Adverse-Effect' in line else 0 for line in lines]\n","\n","# Split the dataset into training and testing sets\n","sentences_train, sentences_test, labels_train, labels_test = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n","\n","# Define a simple dataset class\n","class ADEDataset(Dataset):\n","    def __init__(self, sentences, labels, tokenizer):\n","        self.sentences = sentences\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.sentences)\n","\n","    def __getitem__(self, idx):\n","        sentence = self.sentences[idx]\n","        label = self.labels[idx]\n","        tokens = self.tokenizer(sentence)  # Assuming you have a tokenizer function\n","        return {'tokens': tokens, 'label': label}\n","\n","# Example tokenizer function\n","def tokenize(sentence):\n","    # Implement your own tokenizer (e.g., using spaCy, nltk, or simple split)\n","    return sentence.split()\n","\n","# Hyperparameters\n","embedding_dim = 100\n","hidden_dim = 128\n","output_dim = 1\n","batch_size = 32\n","learning_rate = 0.001\n","num_epochs = 5\n","\n","# Model architecture\n","class ADEModel(nn.Module):\n","    def __init__(self, embedding_dim, hidden_dim, output_dim, rnn_type='lstm'):\n","        super(ADEModel, self).__init__()\n","        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n","        if rnn_type == 'lstm':\n","            self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n","        elif rnn_type == 'bilstm':\n","            self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n","        elif rnn_type == 'gru':\n","            self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        rnn_out, _ = self.rnn(embedded)\n","        output = self.fc(rnn_out[:, -1, :])\n","        output = self.sigmoid(output)\n","        return output\n","\n","# Training loop\n","def train_model(model, train_loader, criterion, optimizer, num_epochs):\n","    model.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0.0\n","        for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}'):\n","            tokens = batch['tokens']\n","            labels = batch['label'].float()\n","\n","            optimizer.zero_grad()\n","            output = model(tokens)\n","            loss = criterion(output, labels.unsqueeze(1))\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        print(f'Training Loss: {avg_loss:.4f}')\n","\n","# Evaluation function\n","def evaluate_model(model, test_loader, criterion):\n","    model.eval()\n","    all_labels = []\n","    all_preds = []\n","    total_loss = 0.0\n","    with torch.no_grad():\n","        for batch in tqdm(test_loader, desc='Evaluating'):\n","            tokens = batch['tokens']\n","            labels = batch['label'].float()\n","\n","            output = model(tokens)\n","            loss = criterion(output, labels.unsqueeze(1))\n","            total_loss += loss.item()\n","\n","            preds = output.cpu().numpy()\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(preds)\n","\n","    avg_loss = total_loss / len(test_loader)\n","    y_true = [1 if label > 0.5 else 0 for label in all_labels]\n","    y_pred = [1 if pred > 0.5 else 0 for pred in all_preds]\n","\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","    roc_auc = roc_auc_score(y_true, all_preds)\n","\n","    print(f'Evaluation Loss: {avg_loss:.4f}')\n","    print(f'Accuracy: {accuracy:.4f}')\n","    print(f'Precision: {precision:.4f}')\n","    print(f'Recall: {recall:.4f}')\n","    print(f'F1 Score: {f1:.4f}')\n","    print(f'AUC-ROC: {roc_auc:.4f}')\n","\n","# Instantiate the model, criterion, and optimizer\n","model_lstm = ADEModel(embedding_dim, hidden_dim, output_dim, rnn_type='lstm')\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model_lstm.parameters(), lr=learning_rate)\n","\n","# Create datasets and loaders\n","train_dataset = ADEDataset(sentences_train, labels_train, tokenize)\n","test_dataset = ADEDataset(sentences_test, labels_test, tokenize)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Train and evaluate the LSTM model\n","train_model(model_lstm, train_loader, criterion, optimizer, num_epochs)\n","evaluate_model(model_lstm, test_loader, criterion)\n"],"metadata":{"id":"9haA1-YW9e9G"},"execution_count":null,"outputs":[]}]}