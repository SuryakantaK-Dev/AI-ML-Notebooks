{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPBtRXOYOmJF933P7kQsasT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"id":"eBAVDs5C3Me5","executionInfo":{"status":"ok","timestamp":1726944238879,"user_tz":-330,"elapsed":2,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}}},"outputs":[],"source":["# Remove the directory if it exists\n","!rm -rf multimodal"]},{"cell_type":"code","source":["!wget https://github.com/facebookresearch/multimodal/archive/refs/heads/main.zip -O multimodal.zip\n","\n","# Step 2: Unzip the downloaded file\n","!unzip multimodal.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AYsxx0L5i11","executionInfo":{"status":"ok","timestamp":1726944264863,"user_tz":-330,"elapsed":2102,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"59ce481c-48ea-49bd-99bf-468ae2fe6719"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-09-21 18:44:21--  https://github.com/facebookresearch/multimodal/archive/refs/heads/main.zip\n","Resolving github.com (github.com)... 140.82.113.4\n","Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://codeload.github.com/facebookresearch/multimodal/zip/refs/heads/main [following]\n","--2024-09-21 18:44:21--  https://codeload.github.com/facebookresearch/multimodal/zip/refs/heads/main\n","Resolving codeload.github.com (codeload.github.com)... 140.82.113.9\n","Connecting to codeload.github.com (codeload.github.com)|140.82.113.9|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [application/zip]\n","Saving to: ‘multimodal.zip’\n","\n","multimodal.zip          [    <=>             ]   8.25M  11.1MB/s    in 0.7s    \n","\n","2024-09-21 18:44:22 (11.1 MB/s) - ‘multimodal.zip’ saved [8654063]\n","\n","Archive:  multimodal.zip\n","e4d288b45b89cee462a21ab264405f3f368adc21\n","   creating: multimodal-main/\n","  inflating: multimodal-main/.coveragerc  \n","  inflating: multimodal-main/.flake8  \n","   creating: multimodal-main/.github/\n","  inflating: multimodal-main/.github/ISSUE_TEMPLATE.md  \n","   creating: multimodal-main/.github/ISSUE_TEMPLATE/\n"," extracting: multimodal-main/.github/ISSUE_TEMPLATE/config.yml  \n","  inflating: multimodal-main/.github/ISSUE_TEMPLATE/feature-request.yml  \n","  inflating: multimodal-main/.github/PULL_REQUEST_TEMPLATE.md  \n","   creating: multimodal-main/.github/workflows/\n","  inflating: multimodal-main/.github/workflows/lint.yaml  \n","  inflating: multimodal-main/.github/workflows/nightly_build.yaml  \n","  inflating: multimodal-main/.github/workflows/unit_test.yaml  \n","  inflating: multimodal-main/.gitignore  \n","  inflating: multimodal-main/.pre-commit-config.yaml  \n"," extracting: multimodal-main/CHANGELOG.md  \n","  inflating: multimodal-main/CODE_OF_CONDUCT.md  \n","  inflating: multimodal-main/CONTRIBUTING.md  \n","  inflating: multimodal-main/LICENSE  \n","  inflating: multimodal-main/NOTICES  \n","  inflating: multimodal-main/README.md  \n"," extracting: multimodal-main/codecov.yml  \n","  inflating: multimodal-main/dev-requirements.txt  \n","   creating: multimodal-main/docs/\n","  inflating: multimodal-main/docs/license_header.txt  \n","   creating: multimodal-main/examples/\n","  inflating: multimodal-main/examples/__init__.py  \n","   creating: multimodal-main/examples/albef/\n","  inflating: multimodal-main/examples/albef/README.md  \n","  inflating: multimodal-main/examples/albef/__init__.py  \n","   creating: multimodal-main/examples/albef/configs/\n","  inflating: multimodal-main/examples/albef/configs/retrieval.yaml  \n","  inflating: multimodal-main/examples/albef/configs/vqa.yaml  \n","   creating: multimodal-main/examples/albef/data/\n","  inflating: multimodal-main/examples/albef/data/__init__.py  \n","  inflating: multimodal-main/examples/albef/data/retrieval_datamodule.py  \n","  inflating: multimodal-main/examples/albef/data/retrieval_dataset.py  \n","  inflating: multimodal-main/examples/albef/data/transforms.py  \n","  inflating: multimodal-main/examples/albef/data/vqa_datamodules.py  \n","  inflating: multimodal-main/examples/albef/data/vqa_dataset.py  \n","  inflating: multimodal-main/examples/albef/finetune_retrieval.py  \n","  inflating: multimodal-main/examples/albef/finetune_vqa.py  \n","  inflating: multimodal-main/examples/albef/model.py  \n","  inflating: multimodal-main/examples/albef/requirements.txt  \n","  inflating: multimodal-main/examples/albef/retrieval_with_albef.ipynb  \n","  inflating: multimodal-main/examples/albef/utils.py  \n","  inflating: multimodal-main/examples/albef/vqa_with_albef.ipynb  \n","   creating: multimodal-main/examples/cnn_lstm/\n","  inflating: multimodal-main/examples/cnn_lstm/cnn_encoder.py  \n","  inflating: multimodal-main/examples/cnn_lstm/cnn_lstm.py  \n","  inflating: multimodal-main/examples/cnn_lstm/lstm_encoder.py  \n","   creating: multimodal-main/examples/cnn_lstm/tests/\n","  inflating: multimodal-main/examples/cnn_lstm/tests/test_cnn_encoder.py  \n","  inflating: multimodal-main/examples/cnn_lstm/tests/test_cnn_lstm.py  \n","  inflating: multimodal-main/examples/cnn_lstm/tests/test_lstm_encoder.py  \n","   creating: multimodal-main/examples/common/\n","  inflating: multimodal-main/examples/common/__init__.py  \n","   creating: multimodal-main/examples/common/data/\n","  inflating: multimodal-main/examples/common/data/__init__.py  \n","  inflating: multimodal-main/examples/common/data/multidata.py  \n","   creating: multimodal-main/examples/flava/\n","  inflating: multimodal-main/examples/flava/README.md  \n","  inflating: multimodal-main/examples/flava/__init__.py  \n","   creating: multimodal-main/examples/flava/callbacks/\n","  inflating: multimodal-main/examples/flava/callbacks/__init__.py  \n","  inflating: multimodal-main/examples/flava/callbacks/multimodal_eval.py  \n","  inflating: multimodal-main/examples/flava/coco_zero_shot.py  \n","   creating: multimodal-main/examples/flava/configs/\n","   creating: multimodal-main/examples/flava/configs/finetuning/\n","  inflating: multimodal-main/examples/flava/configs/finetuning/qnli.yaml  \n","  inflating: multimodal-main/examples/flava/configs/finetuning/rendered_sst2.yaml  \n","   creating: multimodal-main/examples/flava/configs/pretraining/\n","  inflating: multimodal-main/examples/flava/configs/pretraining/debug.yaml  \n","   creating: multimodal-main/examples/flava/data/\n","  inflating: multimodal-main/examples/flava/data/__init__.py  \n","  inflating: multimodal-main/examples/flava/data/datamodules.py  \n","  inflating: multimodal-main/examples/flava/data/imagenet_zeroshot_data.py  \n","  inflating: multimodal-main/examples/flava/data/transforms.py  \n","  inflating: multimodal-main/examples/flava/data/utils.py  \n","  inflating: multimodal-main/examples/flava/definitions.py  \n","  inflating: multimodal-main/examples/flava/finetune.py  \n","  inflating: multimodal-main/examples/flava/model.py  \n","   creating: multimodal-main/examples/flava/native/\n","  inflating: multimodal-main/examples/flava/native/README.md  \n","  inflating: multimodal-main/examples/flava/native/__init__.py  \n","   creating: multimodal-main/examples/flava/native/configs/\n","  inflating: multimodal-main/examples/flava/native/configs/1.8b.yaml  \n","  inflating: multimodal-main/examples/flava/native/configs/10b.yaml  \n","  inflating: multimodal-main/examples/flava/native/configs/2.7b.yaml  \n","  inflating: multimodal-main/examples/flava/native/configs/4.8b.yaml  \n","  inflating: multimodal-main/examples/flava/native/configs/900m.yaml  \n","  inflating: multimodal-main/examples/flava/native/configs/pretrain_debug.yaml  \n","  inflating: multimodal-main/examples/flava/native/data.py  \n","  inflating: multimodal-main/examples/flava/native/model.py  \n","  inflating: multimodal-main/examples/flava/native/train.py  \n","  inflating: multimodal-main/examples/flava/native/utils.py  \n","   creating: multimodal-main/examples/flava/notebooks/\n","  inflating: multimodal-main/examples/flava/notebooks/RemapFLAVACheckpoint.ipynb  \n","  inflating: multimodal-main/examples/flava/requirements.txt  \n","   creating: multimodal-main/examples/flava/tools/\n","  inflating: multimodal-main/examples/flava/tools/convert_weights.py  \n","  inflating: multimodal-main/examples/flava/train.py  \n","  inflating: multimodal-main/examples/flava/utils.py  \n","   creating: multimodal-main/examples/mdetr/\n","  inflating: multimodal-main/examples/mdetr/LoadAndComparePretrainedWeights.ipynb  \n","  inflating: multimodal-main/examples/mdetr/MDETRTutorial.ipynb  \n","  inflating: multimodal-main/examples/mdetr/README.md  \n","   creating: multimodal-main/examples/mdetr/data/\n","  inflating: multimodal-main/examples/mdetr/data/__init__.py  \n","  inflating: multimodal-main/examples/mdetr/data/datamodule.py  \n","  inflating: multimodal-main/examples/mdetr/data/dataset.py  \n","  inflating: multimodal-main/examples/mdetr/data/flickr_eval.py  \n","  inflating: multimodal-main/examples/mdetr/data/postprocessors.py  \n","  inflating: multimodal-main/examples/mdetr/data/transforms.py  \n","  inflating: multimodal-main/examples/mdetr/loss.py  \n","  inflating: multimodal-main/examples/mdetr/matcher.py  \n","  inflating: multimodal-main/examples/mdetr/optimizer.py  \n","  inflating: multimodal-main/examples/mdetr/phrase_grounding.json  \n","  inflating: multimodal-main/examples/mdetr/phrase_grounding.py  \n","  inflating: multimodal-main/examples/mdetr/requirements.txt  \n","   creating: multimodal-main/examples/mdetr/tests/\n","  inflating: multimodal-main/examples/mdetr/tests/test_loss.py  \n","  inflating: multimodal-main/examples/mdetr/tests/test_matcher.py  \n","  inflating: multimodal-main/examples/mdetr/tests/test_postprocessors.py  \n","   creating: multimodal-main/examples/mdetr/utils/\n","  inflating: multimodal-main/examples/mdetr/utils/__init__.py  \n","  inflating: multimodal-main/examples/mdetr/utils/args_parse.py  \n","  inflating: multimodal-main/examples/mdetr/utils/dist.py  \n","  inflating: multimodal-main/examples/mdetr/utils/metrics.py  \n","  inflating: multimodal-main/examples/mdetr/utils/misc.py  \n","  inflating: multimodal-main/examples/mdetr/vqa.json  \n","  inflating: multimodal-main/examples/mdetr/vqa_eval.py  \n","  inflating: multimodal-main/examples/mdetr/vqa_finetune.py  \n","   creating: multimodal-main/examples/mugen/\n","  inflating: multimodal-main/examples/mugen/README.md  \n","   creating: multimodal-main/examples/mugen/data/\n","  inflating: multimodal-main/examples/mugen/data/CHANGELOG.md  \n","  inflating: multimodal-main/examples/mugen/data/README.md  \n","  inflating: multimodal-main/examples/mugen/data/audio_utils.py  \n","  inflating: multimodal-main/examples/mugen/data/bert_text_transform.py  \n","   creating: multimodal-main/examples/mugen/data/coinrun/\n","  inflating: multimodal-main/examples/mugen/data/coinrun/construct_from_json.py  \n","  inflating: multimodal-main/examples/mugen/data/coinrun/game.py  \n","  inflating: multimodal-main/examples/mugen/data/coinrun/generate_text_desc.py  \n","  inflating: multimodal-main/examples/mugen/data/mugen_datamodules.py  \n","  inflating: multimodal-main/examples/mugen/data/mugen_dataset.py  \n","  inflating: multimodal-main/examples/mugen/data/video_utils.py  \n","   creating: multimodal-main/examples/mugen/generation/\n","  inflating: multimodal-main/examples/mugen/generation/LoadAndComparePretrainedVQVAE.ipynb  \n","  inflating: multimodal-main/examples/mugen/generation/README.md  \n","  inflating: multimodal-main/examples/mugen/generation/text_video_gpt.py  \n","  inflating: multimodal-main/examples/mugen/generation/video_vqvae.py  \n","  inflating: multimodal-main/examples/mugen/requirements.txt  \n","   creating: multimodal-main/examples/mugen/retrieval/\n","  inflating: multimodal-main/examples/mugen/retrieval/README.md  \n","   creating: multimodal-main/examples/mugen/retrieval/configs/\n","  inflating: multimodal-main/examples/mugen/retrieval/configs/eval.yaml  \n","  inflating: multimodal-main/examples/mugen/retrieval/configs/train.yaml  \n","  inflating: multimodal-main/examples/mugen/retrieval/definitions.py  \n","  inflating: multimodal-main/examples/mugen/retrieval/eval.py  \n","  inflating: multimodal-main/examples/mugen/retrieval/model.py  \n","  inflating: multimodal-main/examples/mugen/retrieval/train.py  \n","  inflating: multimodal-main/examples/mugen/retrieval/tutorial.ipynb  \n","  inflating: multimodal-main/examples/mugen/retrieval/video_clip.py  \n","   creating: multimodal-main/examples/mugen/tests/\n","   creating: multimodal-main/examples/mugen/tests/data/\n","  inflating: multimodal-main/examples/mugen/tests/data/__init__.py  \n","  inflating: multimodal-main/examples/mugen/tests/data/test_bert_text_transform.py  \n","   creating: multimodal-main/examples/mugen/tests/generation/\n","  inflating: multimodal-main/examples/mugen/tests/generation/test_text_video_gpt.py  \n","  inflating: multimodal-main/examples/mugen/tests/generation/test_video_vqvae.py  \n","   creating: multimodal-main/examples/mugen/tests/retrieval/\n","  inflating: multimodal-main/examples/mugen/tests/retrieval/test_video_clip.py  \n","   creating: multimodal-main/examples/omnivore/\n"," extracting: multimodal-main/examples/omnivore/.gitignore  \n","  inflating: multimodal-main/examples/omnivore/LoadOriginalPretrainedWeightAndCompare.ipynb  \n","  inflating: multimodal-main/examples/omnivore/README.md  \n","  inflating: multimodal-main/examples/omnivore/__init__.py  \n","   creating: multimodal-main/examples/omnivore/data/\n","  inflating: multimodal-main/examples/omnivore/data/__init__.py  \n","  inflating: multimodal-main/examples/omnivore/data/data_builder.py  \n","  inflating: multimodal-main/examples/omnivore/data/datasets.py  \n","  inflating: multimodal-main/examples/omnivore/data/presets.py  \n","  inflating: multimodal-main/examples/omnivore/data/rand_aug3d.py  \n","  inflating: multimodal-main/examples/omnivore/data/transforms.py  \n","  inflating: multimodal-main/examples/omnivore/omnivore_inference_demo.ipynb  \n","  inflating: multimodal-main/examples/omnivore/train.py  \n","  inflating: multimodal-main/examples/omnivore/utils.py  \n","   creating: multimodal-main/examples/utils/\n","  inflating: multimodal-main/examples/utils/common.py  \n","  inflating: multimodal-main/mypy.ini  \n"," extracting: multimodal-main/pyproject.toml  \n"," extracting: multimodal-main/requirements.txt  \n","  inflating: multimodal-main/setup.py  \n","   creating: multimodal-main/tests/\n","  inflating: multimodal-main/tests/__init__.py  \n","   creating: multimodal-main/tests/assets/\n","  inflating: multimodal-main/tests/assets/S3D_sample.pt  \n","  inflating: multimodal-main/tests/assets/clip_vocab.bpe  \n","  inflating: multimodal-main/tests/assets/kaldi_file_8000.wav  \n","  inflating: multimodal-main/tests/assets/sinewave.wav  \n","  inflating: multimodal-main/tests/assets/test_image.jpg  \n","   creating: multimodal-main/tests/diffusion_labs/\n","  inflating: multimodal-main/tests/diffusion_labs/__init__.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_adapter_cfguidance.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_adm.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_adm_blocks.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_adm_crossattention.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_common_util.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_dalle2.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_diffusion_losses.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_diffusion_transform.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_discrete_schedule.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_inpainting_transform.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_predictors.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_sampler_ddim.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_sampler_ddpm.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_super_resolution.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_super_resolution_transform.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_v_transform.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_vae.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_vae_attention.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_vae_encoder_decoder.py  \n","  inflating: multimodal-main/tests/diffusion_labs/test_vae_residual_sampling.py  \n","   creating: multimodal-main/tests/models/\n","  inflating: multimodal-main/tests/models/__init__.py  \n","   creating: multimodal-main/tests/models/albef/\n","  inflating: multimodal-main/tests/models/albef/__init__.py  \n","  inflating: multimodal-main/tests/models/albef/test_albef.py  \n","  inflating: multimodal-main/tests/models/albef/test_image_encoder.py  \n","  inflating: multimodal-main/tests/models/albef/test_multimodal_encoder.py  \n","   creating: multimodal-main/tests/models/blip2/\n","  inflating: multimodal-main/tests/models/blip2/__init__.py  \n","  inflating: multimodal-main/tests/models/blip2/test_blip2.py  \n","  inflating: multimodal-main/tests/models/blip2/test_qformer_layers.py  \n","  inflating: multimodal-main/tests/models/blip2/test_qformer_model.py  \n","  inflating: multimodal-main/tests/models/blip2/test_qformer_utils.py  \n","   creating: multimodal-main/tests/models/clip/\n","  inflating: multimodal-main/tests/models/clip/__init__.py  \n","  inflating: multimodal-main/tests/models/clip/test_checkpoint.py  \n","  inflating: multimodal-main/tests/models/clip/test_clip.py  \n","  inflating: multimodal-main/tests/models/clip/test_image_encoder.py  \n","  inflating: multimodal-main/tests/models/clip/test_text_encoder.py  \n","   creating: multimodal-main/tests/models/coca/\n","  inflating: multimodal-main/tests/models/coca/__init__.py  \n","  inflating: multimodal-main/tests/models/coca/test_coca_model.py  \n","  inflating: multimodal-main/tests/models/coca/test_multimodal_decoder.py  \n","  inflating: multimodal-main/tests/models/coca/test_text_decoder.py  \n","   creating: multimodal-main/tests/models/flava/\n","  inflating: multimodal-main/tests/models/flava/__init__.py  \n","  inflating: multimodal-main/tests/models/flava/test_checkpoint.py  \n","  inflating: multimodal-main/tests/models/flava/test_flava.py  \n","  inflating: multimodal-main/tests/models/flava/test_image_encoder.py  \n","  inflating: multimodal-main/tests/models/flava/test_text_encoder.py  \n","  inflating: multimodal-main/tests/models/flava/test_transformer.py  \n","   creating: multimodal-main/tests/models/masked_auto_encoder/\n","  inflating: multimodal-main/tests/models/masked_auto_encoder/__init__.py  \n","  inflating: multimodal-main/tests/models/masked_auto_encoder/test_model.py  \n","  inflating: multimodal-main/tests/models/masked_auto_encoder/test_position_embeddings.py  \n","  inflating: multimodal-main/tests/models/masked_auto_encoder/test_swin_decoder.py  \n","  inflating: multimodal-main/tests/models/masked_auto_encoder/test_utils.py  \n","   creating: multimodal-main/tests/models/mdetr/\n","  inflating: multimodal-main/tests/models/mdetr/__init__.py  \n","  inflating: multimodal-main/tests/models/mdetr/test_image_encoder.py  \n","  inflating: multimodal-main/tests/models/mdetr/test_mdetr.py  \n","  inflating: multimodal-main/tests/models/mdetr/test_text_encoder.py  \n","  inflating: multimodal-main/tests/models/mdetr/test_transformer.py  \n","  inflating: multimodal-main/tests/models/test_gpt.py  \n","  inflating: multimodal-main/tests/models/test_late_fusion.py  \n","  inflating: multimodal-main/tests/models/test_omnivore.py  \n","  inflating: multimodal-main/tests/models/test_two_tower.py  \n","  inflating: multimodal-main/tests/models/test_video_gpt.py  \n","  inflating: multimodal-main/tests/models/test_video_vqvae.py  \n","  inflating: multimodal-main/tests/models/test_vqvae.py  \n","   creating: multimodal-main/tests/modules/\n","  inflating: multimodal-main/tests/modules/__init__.py  \n","   creating: multimodal-main/tests/modules/encoders/\n","  inflating: multimodal-main/tests/modules/encoders/__init__.py  \n","  inflating: multimodal-main/tests/modules/encoders/test_bert_text_encoder.py  \n","  inflating: multimodal-main/tests/modules/encoders/test_embedding_encoder.py  \n","  inflating: multimodal-main/tests/modules/encoders/test_mil_encoder.py  \n","  inflating: multimodal-main/tests/modules/encoders/test_swin_transformer_3d_encoder.py  \n","  inflating: multimodal-main/tests/modules/encoders/test_weighted_embedding_encoder.py  \n","   creating: multimodal-main/tests/modules/fusions/\n","  inflating: multimodal-main/tests/modules/fusions/__init__.py  \n","  inflating: multimodal-main/tests/modules/fusions/test_attention_fusion.py  \n","  inflating: multimodal-main/tests/modules/fusions/test_deepset_fusion.py  \n","   creating: multimodal-main/tests/modules/layers/\n","  inflating: multimodal-main/tests/modules/layers/__init__.py  \n","  inflating: multimodal-main/tests/modules/layers/test_activation.py  \n","  inflating: multimodal-main/tests/modules/layers/test_anyprecision_optimizer.py  \n","  inflating: multimodal-main/tests/modules/layers/test_attention.py  \n","  inflating: multimodal-main/tests/modules/layers/test_attention_pooler.py  \n","  inflating: multimodal-main/tests/modules/layers/test_codebook.py  \n","  inflating: multimodal-main/tests/modules/layers/test_conv.py  \n","  inflating: multimodal-main/tests/modules/layers/test_mlp.py  \n","  inflating: multimodal-main/tests/modules/layers/test_multi_head_attention.py  \n","  inflating: multimodal-main/tests/modules/layers/test_normalizations.py  \n","  inflating: multimodal-main/tests/modules/layers/test_patch_embedding.py  \n","  inflating: multimodal-main/tests/modules/layers/test_position_embedding.py  \n","  inflating: multimodal-main/tests/modules/layers/test_text_embedding.py  \n","  inflating: multimodal-main/tests/modules/layers/test_transformer.py  \n","   creating: multimodal-main/tests/modules/losses/\n","  inflating: multimodal-main/tests/modules/losses/__init__.py  \n","  inflating: multimodal-main/tests/modules/losses/test_albef.py  \n","  inflating: multimodal-main/tests/modules/losses/test_blip2_loss.py  \n","  inflating: multimodal-main/tests/modules/losses/test_commitment.py  \n","  inflating: multimodal-main/tests/modules/losses/test_contrastive_loss_with_temperature.py  \n","  inflating: multimodal-main/tests/modules/losses/test_mdetr_losses.py  \n","  inflating: multimodal-main/tests/modules/losses/test_reconstruction_loss.py  \n","   creating: multimodal-main/tests/modules/masking/\n","  inflating: multimodal-main/tests/modules/masking/__init__.py  \n","  inflating: multimodal-main/tests/modules/masking/test_random_masking.py  \n","  inflating: multimodal-main/tests/test_utils.py  \n","   creating: multimodal-main/tests/transforms/\n","  inflating: multimodal-main/tests/transforms/__init__.py  \n","  inflating: multimodal-main/tests/transforms/test_clip_transform.py  \n","  inflating: multimodal-main/tests/transforms/test_flava_transform.py  \n","  inflating: multimodal-main/tests/transforms/test_mae_transform.py  \n","  inflating: multimodal-main/tests/transforms/test_text_transforms.py  \n","  inflating: multimodal-main/tests/transforms/test_video_transform.py  \n","   creating: multimodal-main/tests/utils/\n","  inflating: multimodal-main/tests/utils/__init__.py  \n","  inflating: multimodal-main/tests/utils/test_assertion.py  \n","  inflating: multimodal-main/tests/utils/test_attention_utils.py  \n","  inflating: multimodal-main/tests/utils/test_ckpt_load.py  \n","  inflating: multimodal-main/tests/utils/test_common.py  \n","  inflating: multimodal-main/tests/utils/test_distributed.py  \n","  inflating: multimodal-main/tests/utils/test_generate.py  \n","   creating: multimodal-main/torchmultimodal/\n","  inflating: multimodal-main/torchmultimodal/__init__.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/README.md  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/mnist_training.ipynb  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/models/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/__init__.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/models/adm_unet/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/adm_unet/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/adm_unet/adm.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/adm_unet/attention_block.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/adm_unet/res_block.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/models/dalle2/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/dalle2/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/dalle2/dalle2_decoder.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/dalle2/transforms.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/models/vae/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/vae/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/vae/attention.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/vae/encoder_decoder.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/vae/res_block.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/vae/residual_sampling.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/models/vae/vae.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/modules/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/modules/__init__.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/modules/adapters/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/modules/adapters/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/modules/adapters/adapter.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/modules/adapters/cfguidance.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/modules/adapters/inpainting.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/modules/adapters/super_resolution.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/modules/losses/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/modules/losses/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/modules/losses/diffusion_hybrid_loss.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/modules/losses/vlb_loss.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/predictors/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/predictors/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/predictors/noise_predictor.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/predictors/predictor.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/predictors/target_predictor.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/predictors/v_predictor.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/samplers/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/samplers/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/samplers/ddim.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/samplers/ddpm.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/samplers/sampler.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/schedules/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/schedules/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/schedules/discrete_gaussian_schedule.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/schedules/schedule.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/transforms/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/transforms/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/transforms/diffusion_transform.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/transforms/inpainting_transform.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/transforms/super_resolution_transform.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/transforms/v_transform.py  \n","   creating: multimodal-main/torchmultimodal/diffusion_labs/utils/\n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/utils/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/diffusion_labs/utils/common.py  \n","   creating: multimodal-main/torchmultimodal/models/\n","  inflating: multimodal-main/torchmultimodal/models/__init__.py  \n","   creating: multimodal-main/torchmultimodal/models/albef/\n","  inflating: multimodal-main/torchmultimodal/models/albef/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/models/albef/image_encoder.py  \n","  inflating: multimodal-main/torchmultimodal/models/albef/model.py  \n","  inflating: multimodal-main/torchmultimodal/models/albef/multimodal_encoder.py  \n","   creating: multimodal-main/torchmultimodal/models/blip2/\n","  inflating: multimodal-main/torchmultimodal/models/blip2/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/models/blip2/blip2.py  \n","  inflating: multimodal-main/torchmultimodal/models/blip2/qformer_layers.py  \n","  inflating: multimodal-main/torchmultimodal/models/blip2/qformer_model.py  \n","  inflating: multimodal-main/torchmultimodal/models/blip2/qformer_utils.py  \n","   creating: multimodal-main/torchmultimodal/models/clip/\n","  inflating: multimodal-main/torchmultimodal/models/clip/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/models/clip/image_encoder.py  \n","  inflating: multimodal-main/torchmultimodal/models/clip/model.py  \n","  inflating: multimodal-main/torchmultimodal/models/clip/text_encoder.py  \n","   creating: multimodal-main/torchmultimodal/models/coca/\n","  inflating: multimodal-main/torchmultimodal/models/coca/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/models/coca/coca_model.py  \n","  inflating: multimodal-main/torchmultimodal/models/coca/multimodal_decoder.py  \n","  inflating: multimodal-main/torchmultimodal/models/coca/text_decoder.py  \n","   creating: multimodal-main/torchmultimodal/models/flava/\n","  inflating: multimodal-main/torchmultimodal/models/flava/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/models/flava/image_encoder.py  \n","  inflating: multimodal-main/torchmultimodal/models/flava/model.py  \n","  inflating: multimodal-main/torchmultimodal/models/flava/text_encoder.py  \n","  inflating: multimodal-main/torchmultimodal/models/flava/transformer.py  \n","  inflating: multimodal-main/torchmultimodal/models/late_fusion.py  \n","   creating: multimodal-main/torchmultimodal/models/masked_auto_encoder/\n","  inflating: multimodal-main/torchmultimodal/models/masked_auto_encoder/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/models/masked_auto_encoder/model.py  \n","  inflating: multimodal-main/torchmultimodal/models/masked_auto_encoder/position_embeddings.py  \n","  inflating: multimodal-main/torchmultimodal/models/masked_auto_encoder/swin_decoder.py  \n","  inflating: multimodal-main/torchmultimodal/models/masked_auto_encoder/utils.py  \n","   creating: multimodal-main/torchmultimodal/models/mdetr/\n","  inflating: multimodal-main/torchmultimodal/models/mdetr/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/models/mdetr/image_encoder.py  \n","  inflating: multimodal-main/torchmultimodal/models/mdetr/model.py  \n","  inflating: multimodal-main/torchmultimodal/models/mdetr/text_encoder.py  \n","  inflating: multimodal-main/torchmultimodal/models/mdetr/transformer.py  \n","  inflating: multimodal-main/torchmultimodal/models/omnivore.py  \n","  inflating: multimodal-main/torchmultimodal/models/two_tower.py  \n","   creating: multimodal-main/torchmultimodal/models/video_gpt/\n","  inflating: multimodal-main/torchmultimodal/models/video_gpt/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/models/video_gpt/gpt.py  \n","  inflating: multimodal-main/torchmultimodal/models/video_gpt/model.py  \n","  inflating: multimodal-main/torchmultimodal/models/video_gpt/video_vqvae.py  \n","  inflating: multimodal-main/torchmultimodal/models/vqvae.py  \n","   creating: multimodal-main/torchmultimodal/modules/\n","  inflating: multimodal-main/torchmultimodal/modules/__init__.py  \n","   creating: multimodal-main/torchmultimodal/modules/encoders/\n","  inflating: multimodal-main/torchmultimodal/modules/encoders/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/modules/encoders/bert_text_encoder.py  \n","  inflating: multimodal-main/torchmultimodal/modules/encoders/embedding_encoder.py  \n","  inflating: multimodal-main/torchmultimodal/modules/encoders/mil_encoder.py  \n","  inflating: multimodal-main/torchmultimodal/modules/encoders/swin_transformer_3d_encoder.py  \n","  inflating: multimodal-main/torchmultimodal/modules/encoders/vision_transformer.py  \n","  inflating: multimodal-main/torchmultimodal/modules/encoders/weighted_embedding_encoder.py  \n","   creating: multimodal-main/torchmultimodal/modules/fusions/\n","  inflating: multimodal-main/torchmultimodal/modules/fusions/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/modules/fusions/attention_fusion.py  \n","  inflating: multimodal-main/torchmultimodal/modules/fusions/concat_fusion.py  \n","  inflating: multimodal-main/torchmultimodal/modules/fusions/deepset_fusion.py  \n","   creating: multimodal-main/torchmultimodal/modules/layers/\n","  inflating: multimodal-main/torchmultimodal/modules/layers/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/activation.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/attention.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/attention_pooler.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/codebook.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/conv.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/mlp.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/multi_head_attention.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/normalizations.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/patch_embedding.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/position_embedding.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/text_embedding.py  \n","  inflating: multimodal-main/torchmultimodal/modules/layers/transformer.py  \n","   creating: multimodal-main/torchmultimodal/modules/losses/\n","  inflating: multimodal-main/torchmultimodal/modules/losses/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/modules/losses/albef.py  \n","  inflating: multimodal-main/torchmultimodal/modules/losses/blip2_losses.py  \n","  inflating: multimodal-main/torchmultimodal/modules/losses/contrastive_loss_with_temperature.py  \n","  inflating: multimodal-main/torchmultimodal/modules/losses/flava.py  \n","  inflating: multimodal-main/torchmultimodal/modules/losses/mdetr.py  \n","  inflating: multimodal-main/torchmultimodal/modules/losses/reconstruction_loss.py  \n","  inflating: multimodal-main/torchmultimodal/modules/losses/vqvae.py  \n","   creating: multimodal-main/torchmultimodal/modules/masking/\n","  inflating: multimodal-main/torchmultimodal/modules/masking/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/modules/masking/random_masking.py  \n","   creating: multimodal-main/torchmultimodal/modules/optimizers/\n","  inflating: multimodal-main/torchmultimodal/modules/optimizers/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/modules/optimizers/anyprecision.py  \n","   creating: multimodal-main/torchmultimodal/transforms/\n","  inflating: multimodal-main/torchmultimodal/transforms/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/transforms/clip_transform.py  \n","  inflating: multimodal-main/torchmultimodal/transforms/flava_transform.py  \n","  inflating: multimodal-main/torchmultimodal/transforms/mae_transform.py  \n","  inflating: multimodal-main/torchmultimodal/transforms/text_transforms.py  \n","  inflating: multimodal-main/torchmultimodal/transforms/video_transform.py  \n","   creating: multimodal-main/torchmultimodal/utils/\n","  inflating: multimodal-main/torchmultimodal/utils/__init__.py  \n","  inflating: multimodal-main/torchmultimodal/utils/assertion.py  \n","  inflating: multimodal-main/torchmultimodal/utils/attention.py  \n","  inflating: multimodal-main/torchmultimodal/utils/common.py  \n","  inflating: multimodal-main/torchmultimodal/utils/distributed.py  \n","  inflating: multimodal-main/torchmultimodal/utils/file_io.py  \n","  inflating: multimodal-main/torchmultimodal/utils/generate.py  \n","  inflating: multimodal-main/version.py  \n"]}]},{"cell_type":"code","source":["!cd /content/multimodal-main/\n","!pip install -e .\n","!cd examples\n","!pip install -r /content/multimodal-main/examples/flava/requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hhuVP_gm5qJl","executionInfo":{"status":"ok","timestamp":1726944458723,"user_tz":-330,"elapsed":31601,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"c5009de9-b773-4a75-9b98-957675059ae8"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Obtaining file:///content\n","\u001b[31mERROR: file:///content does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n","\u001b[0m/bin/bash: line 1: cd: examples: No such file or directory\n","Collecting Pillow==9.3.0 (from -r /content/multimodal-main/examples/flava/requirements.txt (line 1))\n","  Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n","Collecting pytorch-lightning==1.8.6 (from -r /content/multimodal-main/examples/flava/requirements.txt (line 2))\n","  Downloading pytorch_lightning-1.8.6-py3-none-any.whl.metadata (23 kB)\n","Collecting datasets==2.9.0 (from -r /content/multimodal-main/examples/flava/requirements.txt (line 3))\n","  Downloading datasets-2.9.0-py3-none-any.whl.metadata (19 kB)\n","Collecting requests==2.31.0 (from -r /content/multimodal-main/examples/flava/requirements.txt (line 4))\n","  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n","Requirement already satisfied: DALL-E==0.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (0.1)\n","Collecting omegaconf==2.1.2 (from -r /content/multimodal-main/examples/flava/requirements.txt (line 6))\n","  Downloading omegaconf-2.1.2-py3-none-any.whl.metadata (3.6 kB)\n","Collecting hydra-core==1.1.2 (from -r /content/multimodal-main/examples/flava/requirements.txt (line 7))\n","  Downloading hydra_core-1.1.2-py3-none-any.whl.metadata (4.1 kB)\n","Collecting transformers==4.30.0 (from -r /content/multimodal-main/examples/flava/requirements.txt (line 8))\n","  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pycocotools==2.0.4 (from -r /content/multimodal-main/examples/flava/requirements.txt (line 9))\n","  Downloading pycocotools-2.0.4.tar.gz (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from -r /content/multimodal-main/examples/flava/requirements.txt (line 10)) (2.17.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (1.26.4)\n","Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (2.4.1+cu121)\n","Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (4.66.5)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (6.0.2)\n","Requirement already satisfied: fsspec>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (2024.6.1)\n","Collecting tensorboardX>=2.2 (from pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2))\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Collecting torchmetrics>=0.7.0 (from pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2))\n","  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (24.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (4.12.2)\n","Collecting lightning-utilities!=0.4.0,>=0.3.0 (from pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2))\n","  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (14.0.2)\n","Collecting dill<0.3.7 (from datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3))\n","  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (2.1.4)\n","Collecting xxhash (from datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3))\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3))\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (3.10.5)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (0.24.7)\n","Collecting responses<0.19 (from datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3))\n","  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 4)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 4)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 4)) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 4)) (2024.8.30)\n","Requirement already satisfied: blobfile in /usr/local/lib/python3.10/dist-packages (from DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (3.0.0)\n","Requirement already satisfied: mypy in /usr/local/lib/python3.10/dist-packages (from DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (1.11.2)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (7.4.4)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (0.19.1+cu121)\n","Collecting antlr4-python3-runtime==4.8 (from omegaconf==2.1.2->-r /content/multimodal-main/examples/flava/requirements.txt (line 6))\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 8)) (3.16.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 8)) (2024.9.11)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 8))\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 8)) (0.4.5)\n","Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0.4->-r /content/multimodal-main/examples/flava/requirements.txt (line 9)) (3.7.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/multimodal-main/examples/flava/requirements.txt (line 10)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/multimodal-main/examples/flava/requirements.txt (line 10)) (1.64.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/multimodal-main/examples/flava/requirements.txt (line 10)) (3.7)\n","Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/multimodal-main/examples/flava/requirements.txt (line 10)) (3.20.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/multimodal-main/examples/flava/requirements.txt (line 10)) (71.0.4)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/multimodal-main/examples/flava/requirements.txt (line 10)) (1.16.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/multimodal-main/examples/flava/requirements.txt (line 10)) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->-r /content/multimodal-main/examples/flava/requirements.txt (line 10)) (3.0.4)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (23.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (1.11.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (4.0.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r /content/multimodal-main/examples/flava/requirements.txt (line 9)) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r /content/multimodal-main/examples/flava/requirements.txt (line 9)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r /content/multimodal-main/examples/flava/requirements.txt (line 9)) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r /content/multimodal-main/examples/flava/requirements.txt (line 9)) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r /content/multimodal-main/examples/flava/requirements.txt (line 9)) (3.1.4)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.4->-r /content/multimodal-main/examples/flava/requirements.txt (line 9)) (2.8.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (3.1.4)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->-r /content/multimodal-main/examples/flava/requirements.txt (line 10)) (2.1.5)\n","Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.10/dist-packages (from blobfile->DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (3.20.0)\n","Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile->DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (4.9.4)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3))\n","  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mypy->DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (1.0.0)\n","Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy->DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (2.0.1)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (2024.2)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.9.0->-r /content/multimodal-main/examples/flava/requirements.txt (line 3)) (2024.1)\n","Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (2.0.0)\n","Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (1.5.0)\n","Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->DALL-E==0.1->-r /content/multimodal-main/examples/flava/requirements.txt (line 5)) (1.2.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->pytorch-lightning==1.8.6->-r /content/multimodal-main/examples/flava/requirements.txt (line 2)) (1.3.0)\n","Downloading Pillow-9.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pytorch_lightning-1.8.6-py3-none-any.whl (800 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.3/800.3 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-2.9.0-py3-none-any.whl (462 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.31.0-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading omegaconf-2.1.2-py3-none-any.whl (74 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hydra_core-1.1.2-py3-none-any.whl (147 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.4/147.4 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n","Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pycocotools, antlr4-python3-runtime\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for pycocotools \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n","\u001b[31m  ERROR: Failed building wheel for pycocotools\u001b[0m\u001b[31m\n","\u001b[0m  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141213 sha256=85b8806e5a049c4bf518006bf6cce2c805e1e14efbf6df1fbf0cf14be54257df\n","  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n","Successfully built antlr4-python3-runtime\n","Failed to build pycocotools\n","\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pycocotools)\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install /content/multimodal-main/setup.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-DLbqrUs6fzj","executionInfo":{"status":"ok","timestamp":1726944535866,"user_tz":-330,"elapsed":1564,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"958db037-95ee-49e6-c8ca-06b462103cc8"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Invalid requirement: '/content/multimodal-main/setup.py': Expected package name at the start of dependency specifier\n","    /content/multimodal-main/setup.py\n","    ^\n","Hint: It looks like a path. The path does exist. \u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!python -m flava.train config=/content/multimodal-main/examples/flava/configs/pretraining/debug.yaml"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MdyorKOO6qXD","executionInfo":{"status":"ok","timestamp":1726944591549,"user_tz":-330,"elapsed":462,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"9259dc10-1d17-435e-aace-1cdc809a3861"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/bin/python3: Error while finding module specification for 'flava.train' (ModuleNotFoundError: No module named 'flava')\n"]}]}]}