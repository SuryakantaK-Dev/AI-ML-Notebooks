{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNvAQoH4JVernuqa1qUlPNm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Suryakanta Karan (M22AIE207) m22aie207@iitj.ac.in**\n","\n"],"metadata":{"id":"o8nFf0y8_Cj8"}},{"cell_type":"markdown","source":["#Step 1: Fine-tune a ViT model with an image dataset"],"metadata":{"id":"7CK1rj3SSWnx"}},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9GjZLa-SSXI","executionInfo":{"status":"ok","timestamp":1714757006418,"user_tz":-330,"elapsed":575545,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"3c2f626e-8c4f-43c4-fa60-37211793ae75"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 1/15, Loss: 1.2794\n","Epoch 2/15, Loss: 0.4598\n","Epoch 3/15, Loss: 0.3020\n","Epoch 4/15, Loss: 0.0773\n","Epoch 5/15, Loss: 0.0326\n","Epoch 6/15, Loss: 0.0148\n","Epoch 7/15, Loss: 0.0160\n","Epoch 8/15, Loss: 0.0121\n","Epoch 9/15, Loss: 0.0090\n","Epoch 10/15, Loss: 0.0087\n","Epoch 11/15, Loss: 0.0086\n","Epoch 12/15, Loss: 0.0086\n","Epoch 13/15, Loss: 0.0087\n","Epoch 14/15, Loss: 0.0103\n","Epoch 15/15, Loss: 0.0086\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Subset\n","from torchvision import datasets, transforms, models\n","import numpy as np\n","\n","# Define the data transformations\n","transform = transforms.Compose([\n","    transforms.Resize(224),  # Resize the images to 224x224\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalize dataset\n","])\n","\n","# Load the CIFAR-10 dataset\n","train_dataset = datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n","\n","# Sub-sample the training dataset\n","num_samples = 1000  # Number of samples to use for fine-tuning (you can adjust this number)\n","indices = np.random.choice(len(train_dataset), num_samples, replace=False)\n","train_subset = Subset(train_dataset, indices)\n","\n","# Create data loaders\n","train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)  # Reduced batch size\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Load a pre-trained ViT model\n","vit_model = models.vit_b_16(pretrained=True)\n","\n","# Replace the head of the ViT model with a new head for the specific dataset\n","vit_model.heads = nn.Sequential(\n","    nn.Linear(vit_model.heads[-1].in_features, 512),\n","    nn.ReLU(),\n","    nn.Dropout(0.5),  # Add dropout layer for regularization\n","    nn.Linear(512, 10)  # 10 classes for CIFAR-10 dataset\n",")\n","\n","# Check if GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","vit_model.to(device)  # Move the model to the available device\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(vit_model.parameters(), lr=0.0001, weight_decay=0.0001)  # Adjusted learning rate and added weight decay\n","\n","# Define learning rate scheduler\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)  # Reduce learning rate every 3 epochs\n","\n","# Fine-tune the model using the smaller subset of the data\n","num_epochs = 15  # Increased number of epochs\n","vit_model.train()\n","for epoch in range(num_epochs):\n","    epoch_loss = 0\n","    num_batches = 0\n","\n","    for images, labels in train_loader:\n","        # Move images and labels to the device\n","        images, labels = images.to(device), labels.to(device)\n","\n","        # Perform a training step\n","        optimizer.zero_grad()\n","        outputs = vit_model(images)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","\n","        # Gradient clipping to avoid exploding gradients\n","        nn.utils.clip_grad_norm_(vit_model.parameters(), max_norm=1.0)\n","\n","        optimizer.step()\n","\n","        # Update running loss\n","        epoch_loss += loss.item()\n","        num_batches += 1\n","\n","    # Calculate and print average loss for the epoch\n","    average_loss = epoch_loss / num_batches\n","    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss:.4f}\")\n","\n","    # Step the learning rate scheduler\n","    scheduler.step()\n","\n","# Save the fine-tuned model\n","torch.save(vit_model.state_dict(), 'vit_finetuned.pth')\n"]},{"cell_type":"markdown","source":["#2. Save the model to ONNX format"],"metadata":{"id":"bEP0IF8CPdJ6"}},{"cell_type":"code","source":["!pip install onnx --upgrade\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRlVNc0Sncb-","executionInfo":{"status":"ok","timestamp":1714756219972,"user_tz":-330,"elapsed":11155,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"b0291f1d-c14d-4d92-b724-dfa29d1af16a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx\n","  Downloading onnx-1.16.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.16.0\n"]}]},{"cell_type":"code","source":["import torch.onnx\n","\n","# Define the input shape (batch size, channels, height, width)\n","dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Use the same device as the model\n","\n","# Load the fine-tuned model\n","vit_model.load_state_dict(torch.load('vit_finetuned.pth'))\n","vit_model.to(device)  # Move the model to the device\n","\n","# Convert the model to ONNX format with opset_version 13\n","torch.onnx.export(\n","    vit_model,\n","    dummy_input,\n","    'vit_finetuned.onnx',\n","    input_names=['input'],\n","    output_names=['output'],\n","    opset_version=14  # Updated opset version to 14\n","    ,dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # Allow dynamic batch size\n",")\n","\n"],"metadata":{"id":"d2dq0mjDXQso","executionInfo":{"status":"ok","timestamp":1714756226912,"user_tz":-330,"elapsed":6944,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8c6d3d4a-509f-4a1e-f940-4f4a54ef7206"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/__init__.py:1499: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  assert condition, message\n"]}]},{"cell_type":"markdown","source":["#3. Run the ONNX Model with ONNX Runtime"],"metadata":{"id":"PuZOFc_LSPwn"}},{"cell_type":"code","source":["!pip install onnxruntime"],"metadata":{"id":"gC49VKLOX8iF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714756240426,"user_tz":-330,"elapsed":8746,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"0a1da470-04c6-404d-f89d-a4de8f15675d"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnxruntime\n","  Downloading onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.17.3\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","import numpy as np\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Load the ONNX model\n","ort_session = ort.InferenceSession('vit_finetuned.onnx')\n","\n","# Define input transformation\n","transform = transforms.Compose([\n","    transforms.Resize(224),\n","    transforms.ToTensor(),\n","])\n","\n","# Load the CIFAR-10 test dataset\n","test_dataset = datasets.CIFAR10(root='./data', train=False, transform=transform, download=True)\n","\n","# Sub-sample the training dataset\n","num_samples = 200  # Number of samples to use for fine-tuning (you can adjust this number)\n","indices = np.random.choice(len(test_dataset), num_samples, replace=False)  # Randomly select samples\n","test_dataset = Subset(test_dataset, indices)\n","\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","# Run inference with the ONNX model\n","# Adjust the input shape by iterating over the test loader and running inference one batch at a time with batch size of 1\n","for images, labels in test_loader:\n","    # Iterate over each image in the batch\n","    for i in range(len(images)):\n","        # Get a single image\n","        single_image = images[i:i + 1]\n","        single_image_np = single_image.numpy()\n","\n","        # Perform inference with a single image\n","        outputs = ort_session.run(None, {'input': single_image_np})\n","        prediction = np.argmax(outputs[0], axis=1)\n","\n","        # Print or process the prediction as needed\n","        print(f\"Prediction: {prediction}, Ground Truth: {labels[i].item()}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fwFEZlDJSU4S","executionInfo":{"status":"ok","timestamp":1714756366580,"user_tz":-330,"elapsed":126184,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"eef174aa-2375-4235-9b5f-7759e7d22c57"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Prediction: [7], Ground Truth: 7\n","Prediction: [4], Ground Truth: 3\n","Prediction: [6], Ground Truth: 6\n","Prediction: [5], Ground Truth: 5\n","Prediction: [3], Ground Truth: 6\n","Prediction: [8], Ground Truth: 8\n","Prediction: [2], Ground Truth: 2\n","Prediction: [0], Ground Truth: 0\n","Prediction: [3], Ground Truth: 5\n","Prediction: [2], Ground Truth: 2\n","Prediction: [0], Ground Truth: 0\n","Prediction: [8], Ground Truth: 8\n","Prediction: [8], Ground Truth: 8\n","Prediction: [3], Ground Truth: 4\n","Prediction: [7], Ground Truth: 7\n","Prediction: [2], Ground Truth: 2\n","Prediction: [4], Ground Truth: 4\n","Prediction: [1], Ground Truth: 1\n","Prediction: [3], Ground Truth: 5\n","Prediction: [1], Ground Truth: 1\n","Prediction: [3], Ground Truth: 3\n","Prediction: [0], Ground Truth: 5\n","Prediction: [3], Ground Truth: 3\n","Prediction: [6], Ground Truth: 9\n","Prediction: [6], Ground Truth: 6\n","Prediction: [9], Ground Truth: 9\n","Prediction: [9], Ground Truth: 9\n","Prediction: [0], Ground Truth: 2\n","Prediction: [8], Ground Truth: 8\n","Prediction: [5], Ground Truth: 5\n","Prediction: [2], Ground Truth: 2\n","Prediction: [9], Ground Truth: 9\n","Prediction: [5], Ground Truth: 5\n","Prediction: [4], Ground Truth: 7\n","Prediction: [0], Ground Truth: 0\n","Prediction: [4], Ground Truth: 6\n","Prediction: [3], Ground Truth: 3\n","Prediction: [4], Ground Truth: 4\n","Prediction: [2], Ground Truth: 5\n","Prediction: [4], Ground Truth: 4\n","Prediction: [4], Ground Truth: 4\n","Prediction: [8], Ground Truth: 8\n","Prediction: [2], Ground Truth: 3\n","Prediction: [6], Ground Truth: 6\n","Prediction: [8], Ground Truth: 8\n","Prediction: [8], Ground Truth: 8\n","Prediction: [5], Ground Truth: 5\n","Prediction: [3], Ground Truth: 3\n","Prediction: [3], Ground Truth: 3\n","Prediction: [6], Ground Truth: 6\n","Prediction: [2], Ground Truth: 5\n","Prediction: [0], Ground Truth: 0\n","Prediction: [7], Ground Truth: 7\n","Prediction: [6], Ground Truth: 6\n","Prediction: [5], Ground Truth: 5\n","Prediction: [2], Ground Truth: 2\n","Prediction: [1], Ground Truth: 1\n","Prediction: [7], Ground Truth: 7\n","Prediction: [7], Ground Truth: 7\n","Prediction: [5], Ground Truth: 5\n","Prediction: [2], Ground Truth: 6\n","Prediction: [5], Ground Truth: 5\n","Prediction: [9], Ground Truth: 9\n","Prediction: [5], Ground Truth: 5\n","Prediction: [4], Ground Truth: 3\n","Prediction: [6], Ground Truth: 6\n","Prediction: [5], Ground Truth: 5\n","Prediction: [9], Ground Truth: 9\n","Prediction: [0], Ground Truth: 0\n","Prediction: [1], Ground Truth: 1\n","Prediction: [4], Ground Truth: 4\n","Prediction: [7], Ground Truth: 7\n","Prediction: [1], Ground Truth: 1\n","Prediction: [4], Ground Truth: 6\n","Prediction: [9], Ground Truth: 9\n","Prediction: [3], Ground Truth: 5\n","Prediction: [7], Ground Truth: 7\n","Prediction: [0], Ground Truth: 0\n","Prediction: [9], Ground Truth: 9\n","Prediction: [6], Ground Truth: 6\n","Prediction: [9], Ground Truth: 9\n","Prediction: [9], Ground Truth: 9\n","Prediction: [5], Ground Truth: 5\n","Prediction: [2], Ground Truth: 3\n","Prediction: [3], Ground Truth: 5\n","Prediction: [2], Ground Truth: 2\n","Prediction: [7], Ground Truth: 7\n","Prediction: [0], Ground Truth: 0\n","Prediction: [2], Ground Truth: 3\n","Prediction: [8], Ground Truth: 5\n","Prediction: [0], Ground Truth: 0\n","Prediction: [2], Ground Truth: 2\n","Prediction: [9], Ground Truth: 9\n","Prediction: [0], Ground Truth: 5\n","Prediction: [1], Ground Truth: 1\n","Prediction: [3], Ground Truth: 3\n","Prediction: [0], Ground Truth: 0\n","Prediction: [7], Ground Truth: 7\n","Prediction: [7], Ground Truth: 5\n","Prediction: [4], Ground Truth: 4\n","Prediction: [2], Ground Truth: 5\n","Prediction: [5], Ground Truth: 5\n","Prediction: [6], Ground Truth: 6\n","Prediction: [3], Ground Truth: 3\n","Prediction: [0], Ground Truth: 0\n","Prediction: [9], Ground Truth: 9\n","Prediction: [3], Ground Truth: 2\n","Prediction: [6], Ground Truth: 6\n","Prediction: [2], Ground Truth: 2\n","Prediction: [9], Ground Truth: 9\n","Prediction: [5], Ground Truth: 5\n","Prediction: [8], Ground Truth: 8\n","Prediction: [1], Ground Truth: 1\n","Prediction: [1], Ground Truth: 1\n","Prediction: [1], Ground Truth: 1\n","Prediction: [5], Ground Truth: 3\n","Prediction: [5], Ground Truth: 5\n","Prediction: [8], Ground Truth: 1\n","Prediction: [1], Ground Truth: 1\n","Prediction: [0], Ground Truth: 0\n","Prediction: [7], Ground Truth: 7\n","Prediction: [5], Ground Truth: 5\n","Prediction: [2], Ground Truth: 5\n","Prediction: [4], Ground Truth: 4\n","Prediction: [7], Ground Truth: 7\n","Prediction: [5], Ground Truth: 5\n","Prediction: [1], Ground Truth: 1\n","Prediction: [9], Ground Truth: 9\n","Prediction: [1], Ground Truth: 1\n","Prediction: [7], Ground Truth: 7\n","Prediction: [3], Ground Truth: 5\n","Prediction: [4], Ground Truth: 4\n","Prediction: [6], Ground Truth: 6\n","Prediction: [3], Ground Truth: 3\n","Prediction: [0], Ground Truth: 0\n","Prediction: [4], Ground Truth: 4\n","Prediction: [0], Ground Truth: 0\n","Prediction: [2], Ground Truth: 2\n","Prediction: [8], Ground Truth: 8\n","Prediction: [3], Ground Truth: 3\n","Prediction: [3], Ground Truth: 3\n","Prediction: [2], Ground Truth: 3\n","Prediction: [0], Ground Truth: 0\n","Prediction: [5], Ground Truth: 5\n","Prediction: [2], Ground Truth: 3\n","Prediction: [1], Ground Truth: 1\n","Prediction: [1], Ground Truth: 1\n","Prediction: [5], Ground Truth: 5\n","Prediction: [1], Ground Truth: 1\n","Prediction: [3], Ground Truth: 3\n","Prediction: [4], Ground Truth: 4\n","Prediction: [7], Ground Truth: 7\n","Prediction: [9], Ground Truth: 9\n","Prediction: [3], Ground Truth: 3\n","Prediction: [8], Ground Truth: 8\n","Prediction: [4], Ground Truth: 7\n","Prediction: [7], Ground Truth: 5\n","Prediction: [2], Ground Truth: 2\n","Prediction: [5], Ground Truth: 5\n","Prediction: [1], Ground Truth: 1\n","Prediction: [9], Ground Truth: 9\n","Prediction: [2], Ground Truth: 2\n","Prediction: [9], Ground Truth: 1\n","Prediction: [7], Ground Truth: 5\n","Prediction: [9], Ground Truth: 9\n","Prediction: [0], Ground Truth: 6\n","Prediction: [8], Ground Truth: 8\n","Prediction: [0], Ground Truth: 0\n","Prediction: [2], Ground Truth: 2\n","Prediction: [3], Ground Truth: 3\n","Prediction: [9], Ground Truth: 9\n","Prediction: [9], Ground Truth: 1\n","Prediction: [2], Ground Truth: 2\n","Prediction: [8], Ground Truth: 0\n","Prediction: [8], Ground Truth: 8\n","Prediction: [6], Ground Truth: 6\n","Prediction: [0], Ground Truth: 0\n","Prediction: [6], Ground Truth: 6\n","Prediction: [0], Ground Truth: 0\n","Prediction: [0], Ground Truth: 1\n","Prediction: [4], Ground Truth: 4\n","Prediction: [3], Ground Truth: 5\n","Prediction: [7], Ground Truth: 7\n","Prediction: [4], Ground Truth: 4\n","Prediction: [8], Ground Truth: 8\n","Prediction: [2], Ground Truth: 2\n","Prediction: [4], Ground Truth: 4\n","Prediction: [3], Ground Truth: 3\n","Prediction: [6], Ground Truth: 6\n","Prediction: [7], Ground Truth: 7\n","Prediction: [2], Ground Truth: 8\n","Prediction: [5], Ground Truth: 5\n","Prediction: [5], Ground Truth: 5\n","Prediction: [7], Ground Truth: 7\n","Prediction: [1], Ground Truth: 1\n","Prediction: [1], Ground Truth: 1\n","Prediction: [9], Ground Truth: 9\n","Prediction: [2], Ground Truth: 2\n","Prediction: [2], Ground Truth: 2\n","Prediction: [7], Ground Truth: 7\n"]}]},{"cell_type":"markdown","source":["#4. Use TorchScript to Convert Your Code for Inference to a C++ Readable Format\n","To convert the model to TorchScript format:\n","\n","Load the fine-tuned model.\n","Convert the model to TorchScript using the torch.jit.trace or torch.jit.script functions.\n","Save the TorchScript model."],"metadata":{"id":"bn7bEopZWBqf"}},{"cell_type":"code","source":["# Convert the model to TorchScript\n","vit_model_scripted = torch.jit.script(vit_model)\n","vit_model_scripted.save('vit_finetuned_scripted.pt')"],"metadata":{"id":"Bn6CewacWJLe","executionInfo":{"status":"ok","timestamp":1714756428374,"user_tz":-330,"elapsed":6277,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["#5. Load the Torchscript model in C++ for the inference on the test set of your dataset"],"metadata":{"id":"q6TlasttPruL"}},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106},"id":"yS-vv_XeWqL8","executionInfo":{"status":"error","timestamp":1714654196207,"user_tz":-330,"elapsed":10,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"1b3a86d1-dcef-4501-a9e6-a120267f7fd3"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-16-1bf71f993edb>, line 4)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-1bf71f993edb>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    int main() {\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]}]}