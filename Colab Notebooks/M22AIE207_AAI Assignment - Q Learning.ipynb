{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPprZGlF9AW/7LM6AIRd69T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Suryakanta Karan (M22AIE207) m22aie207@iitj.ac.in**\n"],"metadata":{"id":"ExODBf2tJSG2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"67Zo15ehIn9y"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Define the GridWorld environment\n","class GridWorld:\n","    def __init__(self, size=5):\n","        self.size = size\n","        self.grid = np.zeros((size, size))\n","        self.grid[4, 4] = 5  # Goal state\n","        self.grid[1, 3] = -5  # Terminal state\n","        self.actions = ['N', 'S', 'E', 'W']\n","        self.q_values = np.zeros((size, size, len(self.actions)))\n","\n","    def get_reward(self, state):\n","        if state[0] < 0 or state[0] >= self.size or state[1] < 0 or state[1] >= self.size:\n","            return -1\n","        return self.grid[state[0], state[1]]\n","\n","    def is_terminal(self, state):\n","        return self.get_reward(state) in [5, -5]\n","\n","    def step(self, state, action):\n","        if action == 'N':\n","            next_state = (state[0] - 1, state[1])\n","        elif action == 'S':\n","            next_state = (state[0] + 1, state[1])\n","        elif action == 'E':\n","            next_state = (state[0], state[1] + 1)\n","        elif action == 'W':\n","            next_state = (state[0], state[1] - 1)\n","        reward = self.get_reward(next_state)\n","        if self.is_terminal(next_state) or reward == -1:\n","            next_state = state\n","        return next_state, reward\n","\n","# Initialize GridWorld\n","env = GridWorld()\n","\n","# Helper functions\n","def choose_action(state, q_values, epsilon):\n","    if np.random.rand() < epsilon:\n","        return np.random.choice(env.actions)\n","    else:\n","        return env.actions[np.argmax(q_values[state[0], state[1], :])]\n","\n","def update_q_values(state, action, reward, next_state, q_values, alpha, gamma):\n","    action_index = env.actions.index(action)\n","    best_next_action = np.max(q_values[next_state[0], next_state[1], :])\n","    q_values[state[0], state[1], action_index] += alpha * (reward + gamma * best_next_action - q_values[state[0], state[1], action_index])\n","\n","# Training the Q-Learning Agent\n","def train_q_learning(env, alpha=0.1, gamma=0.9, epsilon=0.1, episodes=100000):\n","    q_values = np.zeros((env.size, env.size, len(env.actions)))\n","    steps_to_goal = []\n","\n","    for episode in range(episodes):\n","        state = (0, 0)  # Starting state\n","        steps = 0\n","        while not env.is_terminal(state):\n","            action = choose_action(state, q_values, epsilon)\n","            next_state, reward = env.step(state, action)\n","            update_q_values(state, action, reward, next_state, q_values, alpha, gamma)\n","            state = next_state\n","            steps += 1\n","        steps_to_goal.append(steps)\n","\n","    return q_values, steps_to_goal\n","\n","# Plotting Functions\n","def plot_policy(q_values, env):\n","    policy = np.zeros((env.size, env.size), dtype=str)\n","    for i in range(env.size):\n","        for j in range(env.size):\n","            best_action = np.argmax(q_values[i, j, :])\n","            policy[i, j] = env.actions[best_action]\n","\n","    sns.heatmap(np.max(q_values, axis=2), annot=policy, fmt='', cmap='coolwarm')\n","    plt.title('Policy')\n","    plt.show()\n","\n","def plot_steps(steps, gamma, epsilon):\n","    plt.figure()\n","    plt.plot(steps, label=f'Gamma: {gamma}, Epsilon: {epsilon}')\n","    plt.xscale('log')\n","    plt.xlabel('Episodes')\n","    plt.ylabel('Steps to Goal')\n","    plt.legend()\n","    plt.show()\n","\n","# Training with different parameters and plotting results\n","gammas = [0.1, 0.5, 0.9]\n","epsilons = [0.1, 0.3, 0.5]\n","\n","for gamma in gammas:\n","    q_values, steps = train_q_learning(env, gamma=gamma, epsilon=0.1)\n","    plot_policy(q_values, env)\n","\n","    for epsilon in epsilons:\n","        q_values, steps = train_q_learning(env, gamma=gamma, epsilon=epsilon)\n","        plot_steps(steps, gamma, epsilon)\n"]}]}