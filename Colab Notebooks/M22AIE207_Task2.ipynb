{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOayHkRaGSbO0XQtz1KHnbR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Suryakanta Karan (M22AIE207) m22aie207@iitj.ac.in**"],"metadata":{"id":"T6WmNDx76QMW"}},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/MyDrive/surya/DL_Assignment/Fractal-1_Assignment-1')"],"metadata":{"id":"_5abB_Jx6D8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"id":"zvh1mVqxf9SS","executionInfo":{"status":"error","timestamp":1697729692248,"user_tz":-330,"elapsed":6998,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"947777ee-ca33-4334-dcde-05a72cb1bb22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 0s 0us/step\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-bd1627f98a9d>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpercent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpercentages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Split the dataset based on the percentage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpercent\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Build the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dataset_x' is not defined"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Flatten, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.applications import VGG16\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n","import matplotlib.pyplot as plt\n","\n","# Load and preprocess the STL-10 dataset (You should have this prepared)\n","# You can use your own data loading and preprocessing code here\n","\n","# Load pre-trained autoencoder's encoder (Replace with your encoder model)\n","# For example, let's create a simple encoder with VGG16 as a placeholder\n","encoder_input = Input(shape=(96, 96, 3))\n","encoder_base = VGG16(input_tensor=encoder_input, include_top=False, weights='imagenet')\n","encoder_output = Flatten()(encoder_base.layers[-1].output)\n","encoder_model = Model(encoder_input, encoder_output)\n","\n","# Build the downstream task classifier (MLP)\n","def build_classifier(input_shape, hidden_layers):\n","    input_layer = Input(shape=input_shape)\n","    x = input_layer\n","    for units in hidden_layers:\n","        x = Dense(units, activation='relu')(x)\n","    output_layer = Dense(10, activation='softmax')(x)  # Assuming 10 classes\n","    classifier = Model(input_layer, output_layer)\n","    return classifier\n","\n","# Choose hidden layer configurations\n","hidden_layers_3 = [256, 128, 64]  # Task 2.3.a\n","hidden_layers_5 = [512, 256, 128, 64, 32]  # Task 2.3.b\n","\n","# Initialize variables to store results\n","results = {}\n","\n","# Fine-tune the classifier with different percentages of training samples\n","percentages = [1, 10, 20, 40, 60]  # Task 2.4\n","for percent in percentages:\n","    # Split the dataset based on the percentage\n","    train_x, _, train_y, _ = train_test_split(dataset_x, dataset_y, train_size=percent / 100, stratify=dataset_y)\n","\n","    # Build the classifier\n","    hidden_layers = hidden_layers_3 if percent <= 10 else hidden_layers_5\n","    classifier = build_classifier(encoder_model.output.shape[1:], hidden_layers)\n","\n","    # Compile the classifier\n","    classifier.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    # Fine-tune the classifier on the STL-10 dataset\n","    classifier.fit(train_x, train_y, batch_size=32, epochs=10, validation_split=0.2)\n","\n","    # Evaluate the classifier and store the results\n","    test_x, test_y = load_test_data()  # Load your test data\n","    accuracy = classifier.evaluate(test_x, test_y)[1]\n","    results[percent] = accuracy\n","\n","# Prepare confusion matrix and AUC-ROC curve for selected configurations\n","selected_configurations = [hidden_layers_3, hidden_layers_5]  # Task 2.3.a and 2.3.b\n","for config in selected_configurations:\n","    classifier = build_classifier(encoder_model.output.shape[1:], config)\n","    classifier.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n","    classifier.fit(train_x, train_y, batch_size=32, epochs=10, validation_split=0.2)\n","\n","    # Evaluate the classifier\n","    test_x, test_y = load_test_data()  # Load your test data\n","    predictions = classifier.predict(test_x)\n","    confusion = confusion_matrix(np.argmax(test_y, axis=1), np.argmax(predictions, axis=1))\n","\n","    # Calculate AUC-ROC\n","    fpr, tpr, thresholds = roc_curve(test_y, predictions)\n","    roc_auc = auc(fpr, tpr)\n","\n","    # Store the results for this configuration\n","    results[config] = {'confusion_matrix': confusion, 'roc_auc': roc_auc}\n","\n","# Implement a different architecture (Task 2.6)\n","# You can try different architectures and fine-tuning strategies here\n","\n","# Print or visualize the results as needed\n","print(results)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"s5M2-U6o6Cla"},"execution_count":null,"outputs":[]}]}