{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1r4610g6RErRxs-ViYfoHhnm14zgo50lb","authorship_tag":"ABX9TyNuJ/fe8cbSZd4kbV+dDad1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/MyDrive/surya/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4gImPqzrUg53","executionInfo":{"status":"ok","timestamp":1730016419415,"user_tz":-330,"elapsed":5429,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"3fc63d23-17ef-49b1-ed17-2268dcf22b28"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x7TOuyJaUs6X","executionInfo":{"status":"ok","timestamp":1730016427617,"user_tz":-330,"elapsed":4134,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"20029b43-68c6-4003-f178-fe8a493b6972"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RrtQHX5VTXX9","executionInfo":{"status":"ok","timestamp":1730016461610,"user_tz":-330,"elapsed":22005,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"55b76880-93d7-4373-dd22-4054899db53b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.9.0)\n","Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (1.24.12)\n","Requirement already satisfied: pdf2image in /usr/local/lib/python3.10/dist-packages (1.17.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.13)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.16.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n","Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.10)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: langchain<0.4.0,>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.4)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.13)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.137)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.6.0)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.16.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.23.0)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (0.3.0)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.4->langchain-community) (2.9.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (1.33)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (24.1)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain-community) (4.12.2)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (0.27.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.10)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.8.30)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.6)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain-community) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.4->langchain-community) (2.23.4)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community) (1.2.2)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","poppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"]}],"source":["!pip install transformers faiss-cpu PyMuPDF pdf2image pillow\n","!pip install langchain\n","!pip install -U langchain-community\n","!apt-get install -y poppler-utils\n"]},{"cell_type":"code","source":["import fitz  # PyMuPDF for PDF text extraction\n","from pdf2image import convert_from_path\n","from PIL import Image\n","\n","# Path to your PDF file (upload to Colab first)\n","pdf_path = \"/content/drive/MyDrive/surya/AR_24797_ZOMATO_2023_2024_03082024162627-32_Pages.pdf\"\n","\n","# Initialize lists to hold text and image data\n","pdf_texts = []\n","pdf_images = []\n","\n","# Extract text and images from each page\n","with fitz.open(pdf_path) as pdf:\n","    for page_num in range(pdf.page_count):\n","        page = pdf[page_num]\n","        pdf_texts.append(page.get_text())  # Extract text from page\n","\n","        # Convert page to an image\n","        images = convert_from_path(pdf_path, first_page=page_num + 1, last_page=page_num + 1)\n","        pdf_images.extend(images)  # Store images\n"],"metadata":{"id":"bbUxF2ycTcR_","executionInfo":{"status":"ok","timestamp":1730016461610,"user_tz":-330,"elapsed":4,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from transformers import CLIPProcessor, CLIPModel\n","import torch\n","import numpy as np\n","\n","# Initialize CLIP model and processor for embeddings\n","clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","# Function to split long text into chunks that fit the model's limit\n","def split_text(text, chunk_size=77):\n","    words = text.split()\n","    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n","\n","# Prepare text data by splitting long texts into chunks of 77 tokens or less\n","chunked_texts = []\n","for text in pdf_texts:  # pdf_texts contains extracted text from the PDF\n","    chunked_texts.extend(split_text(text))\n","\n","# Generate embeddings for each text chunk\n","text_inputs = processor(text=chunked_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n","text_embeddings = clip_model.get_text_features(**text_inputs)\n","text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)  # Normalize embeddings\n","\n","# Process and generate image embeddings\n","image_inputs = processor(images=pdf_images, return_tensors=\"pt\")  # pdf_images contains images from PDF\n","image_embeddings = clip_model.get_image_features(**image_inputs)\n","image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)  # Normalize embeddings\n","\n","# Convert embeddings to numpy arrays for FAISS compatibility\n","text_embeddings = text_embeddings.detach().cpu().numpy()\n","image_embeddings = image_embeddings.detach().cpu().numpy()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"O7D3fYCrVSiL","executionInfo":{"status":"error","timestamp":1730016501847,"user_tz":-330,"elapsed":2560,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"50825338-876d-495b-d30c-a9d56e7934d5"},"execution_count":17,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-02f16543563f>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Generate embeddings for each text chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtext_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m77\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_embeddings\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtext_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Normalize embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/clip/processing_clip.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, images, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3053\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_target_context_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_input_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3055\u001b[0;31m             \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3056\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtext_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3057\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_switch_to_target_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m                 )\n\u001b[1;32m   3141\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3142\u001b[0;31m             return self.batch_encode_plus(\n\u001b[0m\u001b[1;32m   3143\u001b[0m                 \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m                 \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3336\u001b[0m         )\n\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m         return self._batch_encode_plus(\n\u001b[0m\u001b[1;32m   3339\u001b[0m             \u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_text_or_text_pairs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m             \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madd_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# we add an overflow_to_sample_mapping array (see below)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0msanitized_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_and_encodings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m             \u001b[0mstack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_and_encodings\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0msanitized_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","source":["from transformers import CLIPProcessor, CLIPModel\n","import torch\n","import numpy as np\n","\n","# Check if CUDA is available and use GPU if possible\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Clear any cached memory from previous sessions\n","torch.cuda.empty_cache()\n","\n","# Attempt to load a model, handling memory issues gracefully\n","clip_model = None\n","\n","try:\n","    # Initialize CLIP model and processor for embeddings\n","    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)  # Move model to GPU\n","except Exception as e:\n","    if \"CUDA out of memory\" in str(e):\n","        print(\"Out of memory! Trying a smaller model.\")\n","        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)  # Try smaller model\n","    else:\n","        raise  # Raise any other exceptions\n","\n","processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","# Debug: Check the extracted PDF texts\n","print(f\"Number of raw texts extracted from PDF: {len(pdf_texts)}\")\n","if len(pdf_texts) == 0:\n","    raise ValueError(\"No text extracted from PDF. Please check the extraction process.\")\n","\n","# Function to split long text into chunks that fit the model's limit\n","def split_text(text, chunk_size=77):\n","    words = text.split()\n","    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n","\n","# Prepare text data by splitting long texts into chunks of 77 tokens or less\n","chunked_texts = []\n","for text in pdf_texts:  # pdf_texts contains extracted text from the PDF\n","    if text:  # Ensure text is not empty\n","        chunks = split_text(text)\n","        chunked_texts.extend(chunks)\n","\n","# Debugging: Check the number of chunked texts\n","print(f\"Number of text chunks: {len(chunked_texts)}\")\n","if len(chunked_texts) == 0:\n","    raise ValueError(\"No text chunks found after processing. Please check the PDF extraction and splitting.\")\n","\n","# Generate embeddings for each text chunk\n","text_inputs = processor(text=chunked_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n","\n","# Move inputs to GPU\n","text_inputs = {key: value.to(device) for key, value in text_inputs.items()}\n","\n","# Generate text embeddings on GPU\n","text_embeddings = clip_model.get_text_features(**text_inputs)\n","text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)  # Normalize embeddings\n","\n","# Process and generate image embeddings\n","image_inputs = processor(images=pdf_images, return_tensors=\"pt\")  # pdf_images contains images from PDF\n","\n","# Move image inputs to GPU\n","image_inputs = {key: value.to(device) for key, value in image_inputs.items()}\n","\n","# Generate image embeddings on GPU\n","image_embeddings = clip_model.get_image_features(**image_inputs)\n","image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)  # Normalize embeddings\n","\n","# Convert embeddings to numpy arrays for FAISS compatibility (move to CPU for conversion)\n","text_embeddings = text_embeddings.detach().cpu().numpy()\n","image_embeddings = image_embeddings.detach().cpu().numpy()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"3jkjXctU993Y","executionInfo":{"status":"error","timestamp":1730016645419,"user_tz":-330,"elapsed":3783,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"c552ee9b-d7e3-4235-d5e8-a8a2747330db"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of raw texts extracted from PDF: 0\n"]},{"output_type":"error","ename":"ValueError","evalue":"No text extracted from PDF. Please check the extraction process.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-130b8fc3f8ad>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Number of raw texts extracted from PDF: {len(pdf_texts)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_texts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No text extracted from PDF. Please check the extraction process.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Function to split long text into chunks that fit the model's limit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No text extracted from PDF. Please check the extraction process."]}]},{"cell_type":"code","source":["!apt-get install tesseract-ocr\n","!pip install pytesseract\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQCJikNWCwn9","executionInfo":{"status":"ok","timestamp":1730016923912,"user_tz":-330,"elapsed":12965,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"5a086306-db58-47ab-8238-19a18136c05e"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  tesseract-ocr-eng tesseract-ocr-osd\n","The following NEW packages will be installed:\n","  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n","0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n","Need to get 4,816 kB of archives.\n","After this operation, 15.6 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n","Fetched 4,816 kB in 2s (2,674 kB/s)\n","Selecting previously unselected package tesseract-ocr-eng.\n","(Reading database ... 123652 files and directories currently installed.)\n","Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n","Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n","Selecting previously unselected package tesseract-ocr-osd.\n","Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n","Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n","Selecting previously unselected package tesseract-ocr.\n","Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n","Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n","Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n","Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n","Setting up tesseract-ocr (4.1.1-2.1build1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Collecting pytesseract\n","  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (10.4.0)\n","Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n","Installing collected packages: pytesseract\n","Successfully installed pytesseract-0.3.13\n"]}]},{"cell_type":"code","source":["from transformers import CLIPProcessor, CLIPModel\n","import torch\n","import numpy as np\n","from pdf2image import convert_from_path\n","from pytesseract import image_to_string\n","import os\n","\n","# Set device to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load PDF file\n","pdf_path = \"/content/drive/MyDrive/surya/AR_24797_ZOMATO_2023_2024_03082024162627-32_Pages.pdf\"\n","\n","# Convert PDF pages to images\n","pdf_images = convert_from_path(pdf_path)\n","\n","# Extract text from images using OCR\n","pdf_texts = []\n","for image in pdf_images:\n","    text = image_to_string(image)\n","    pdf_texts.append(text)\n","\n","# Check if any text was extracted\n","print(f\"Number of raw texts extracted from PDF: {len(pdf_texts)}\")\n","if len(pdf_texts) == 0:\n","    raise ValueError(\"No text extracted from PDF. Please check the extraction process.\")\n","\n","# Initialize CLIP model and processor for embeddings\n","try:\n","    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)  # Move model to GPU\n","    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","except OutOfMemoryError:\n","    print(\"Out of memory! Trying a smaller model.\")\n","    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(device)  # Try smaller model\n","\n","# Function to split long text into chunks that fit the model's limit\n","def split_text(text, chunk_size=77):\n","    words = text.split()\n","    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n","\n","# Prepare text data by splitting long texts into chunks of 77 tokens or less\n","chunked_texts = []\n","for text in pdf_texts:\n","    chunked_texts.extend(split_text(text))\n","\n","print(f\"Number of text chunks: {len(chunked_texts)}\")\n","\n","# Generate embeddings for each text chunk\n","text_inputs = processor(text=chunked_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n","text_inputs = {k: v.to(device) for k, v in text_inputs.items()}  # Move inputs to GPU\n","\n","text_embeddings = clip_model.get_text_features(**text_inputs)\n","text_embeddings = text_embeddings / text_embeddings.norm(dim=-1, keepdim=True)  # Normalize embeddings\n","\n","# Process and generate image embeddings\n","image_inputs = processor(images=pdf_images, return_tensors=\"pt\")  # pdf_images contains images from PDF\n","image_inputs = {k: v.to(device) for k, v in image_inputs.items()}  # Move inputs to GPU\n","\n","image_embeddings = clip_model.get_image_features(**image_inputs)\n","image_embeddings = image_embeddings / image_embeddings.norm(dim=-1, keepdim=True)  # Normalize embeddings\n","\n","# Convert embeddings to numpy arrays for FAISS compatibility\n","text_embeddings = text_embeddings.detach().cpu().numpy()\n","image_embeddings = image_embeddings.detach().cpu().numpy()\n","\n","# Combine text and image embeddings\n","combined_texts = chunked_texts + [\"Image\"] * len(pdf_images)  # Text labels for each embedding\n","combined_embeddings = np.concatenate((text_embeddings, image_embeddings), axis=0)  # Combined embeddings\n","\n","# Initialize FAISS with precomputed embeddings and metadata\n","from langchain.vectorstores import FAISS\n","\n","# Assuming `metadatas` is a list of dictionaries containing metadata for each embedding\n","metadatas = [{\"text\": text} for text in combined_texts]  # Create metadata\n","\n","# Create FAISS vector store\n","vector_store = FAISS.from_embeddings(\n","    text_embeddings=[(text, embedding) for text, embedding in zip(combined_texts, combined_embeddings)],\n","    embedding=combined_embeddings\n",")\n","\n","print(\"FAISS vector store created successfully.\")\n","\n","# Now you can query the vector store using text or images as input.\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"LTo8miNICt1L","executionInfo":{"status":"error","timestamp":1730017020596,"user_tz":-330,"elapsed":490,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"88c23d29-f707-4ef8-c583-3bc1e28b0ff0"},"execution_count":24,"outputs":[{"output_type":"error","ename":"PDFPageCountError","evalue":"Unable to get page count.\nSyntax Error: Couldn't find trailer dictionary\nSyntax Error: Couldn't find trailer dictionary\nSyntax Error: Couldn't read xref table\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"Pages\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mPDFPageCountError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-90f211c22d54>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Convert PDF pages to images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpdf_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Extract text from images using OCR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mconvert_from_path\u001b[0;34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mpoppler_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoppler_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_posix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     page_count = pdfinfo_from_path(\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserpw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mownerpw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoppler_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoppler_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     )[\"Pages\"]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pdf2image/pdf2image.py\u001b[0m in \u001b[0;36mpdfinfo_from_path\u001b[0;34m(pdf_path, userpw, ownerpw, poppler_path, rawdates, timeout, first_page, last_page)\u001b[0m\n\u001b[1;32m    609\u001b[0m         )\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         raise PDFPageCountError(\n\u001b[0m\u001b[1;32m    612\u001b[0m             \u001b[0;34mf\"Unable to get page count.\\n{err.decode('utf8', 'ignore')}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         )\n","\u001b[0;31mPDFPageCountError\u001b[0m: Unable to get page count.\nSyntax Error: Couldn't find trailer dictionary\nSyntax Error: Couldn't find trailer dictionary\nSyntax Error: Couldn't read xref table\n"]}]},{"cell_type":"code","source":["from langchain.vectorstores import FAISS\n","import numpy as np\n","\n","# Combine text and image embeddings and metadata\n","combined_texts = chunked_texts + [\"Image\"] * len(pdf_images)  # Text labels for each embedding\n","combined_embeddings = np.concatenate((text_embeddings, image_embeddings), axis=0)  # Combined embeddings\n","\n","# Initialize FAISS with precomputed embeddings and metadata\n","vector_store = FAISS.from_embeddings(\n","    text_embeddings=[(text, embedding) for text, embedding in zip(combined_texts, combined_embeddings)],\n","    embedding=combined_embeddings\n",")\n","\n","print(\"FAISS vector store created successfully.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9OxKrDhV9DD","executionInfo":{"status":"ok","timestamp":1729854485608,"user_tz":-330,"elapsed":477,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"88e10822-f3a4-4b3b-e02c-1229b9f7cdc4"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"]},{"output_type":"stream","name":"stdout","text":["FAISS vector store created successfully.\n"]}]},{"cell_type":"code","source":["import torch\n","\n","# Function to generate embedding for a query\n","def generate_text_embedding(query):\n","    text_input = processor(text=query, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n","    text_embedding = clip_model.get_text_features(**text_input)\n","    text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)  # Normalize embedding\n","    return text_embedding.detach().cpu().numpy()\n","\n","def generate_image_embedding(image):\n","    image_input = processor(images=image, return_tensors=\"pt\")\n","    image_embedding = clip_model.get_image_features(**image_input)\n","    image_embedding = image_embedding / image_embedding.norm(dim=-1, keepdim=True)  # Normalize embedding\n","    return image_embedding.detach().cpu().numpy()"],"metadata":{"id":"gfqiSV6tXbwO","executionInfo":{"status":"ok","timestamp":1729854556946,"user_tz":-330,"elapsed":678,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Example 1: Text Query\n","text_query = \"Who Is Suryakanta Karan\"  # Example query text\n","text_query_embedding = generate_text_embedding(text_query)\n","text_results = vector_store.similarity_search_by_vector(text_query_embedding[0], k=5)  # Retrieve top 5 matches\n","print(\"Text Query Results:\")\n","for result in text_results:\n","    print(result.page_content)  # Display the retrieved text descriptions\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AWG36U8lXdEU","executionInfo":{"status":"ok","timestamp":1729854622277,"user_tz":-330,"elapsed":643,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"4dbadac0-b822-427a-92db-205f7f5738d6"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Text Query Results:\n","Suryakanta Karan M: +91 8770228646 Email: suryakantakaran93@gmail.com LinkedIn: in/suryakanta-karan-595b4b34/ Professional Summary  Seasoned Sr. Lead Data Scientist with over 10+ years of expertise in developing, fine-tuning, and deploying Large Language Models (LLMs) such as GPT and LLaMA. Proven ability to deliver scalable NLP solutions using cutting-edge techniques including Retrieval-Augmented Generation (RAG), Reinforcement Learning from Human Feedback (RLHF), and advanced prompt engineering. Demonstrated leadership in managing cross-functional teams, mentoring engineers, and driving AI innovations that improve model accuracy\n","Sampling) and Top-K Sampling techniques in cloud environments to optimize text generation models, balancing response quality and computational efficiency\n"," Trained machine learning models (e.g., Decision Trees, Random Forest, and Neural Networks) to classify fraudulent and legitimate transactions, achieving high precision and recall scores.  Employed advanced anomaly detection techniques to identify deviations in transaction patterns, flagging potential fraud cases for review.  Integrated real-time fraud detection mechanisms into banking platforms, ensuring the system could detect and prevent fraudulent transactions as they occur.  Tuned model hyperparameters and adjusted detection thresholds to reduce false positives and\n","and Entity Recognition:  Developed predictive models using machine learning algorithms (e.g., XGBoost, LSTM) to forecast commodity prices, leading to a 15% improvement in decision-making.  Integrated real-time market data using APIs, enabling up-to-date trading insights and enhancing trading strategies.  Designed AI algorithms for risk management, reducing potential losses by analyzing market volatility.  Collaborated with traders and financial analysts to implement an automated trading system, improving trade execution speed and profitability.  Deployed models using\n"," Lead and mentored data science teams to ensure the successful delivery of data science projects, providing guidance on best practices, and ensuring compliance with relevant data privacy and security regulations.  Leveraged Large Language Models (LLMs) for natural language processing (NLP) tasks, such as text preprocessing, feature extraction, Question answering, and sentiment analysis.  Developed End to end RAG LLM App Using Llama2, BART, GPT-3.5, LlamaIndex models with Huggingface and OpenAI.  Implemented data cleanup and\n"]}]},{"cell_type":"code","source":["# Example 2: Image Query\n","image_query = pdf_images[0]  # Example image from PDF\n","image_query_embedding = generate_image_embedding(image_query)\n","image_results = vector_store.similarity_search_by_vector(image_query_embedding[0], k=5)  # Retrieve top 5 matches\n","print(\"\\nImage Query Results:\")\n","for result in image_results:\n","    print(result.page_content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mwrhUPE1XwxG","executionInfo":{"status":"ok","timestamp":1729854723726,"user_tz":-330,"elapsed":988,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"1b0f4514-64b0-42fe-cd7d-9f8e3532e686"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Image Query Results:\n","Image\n","Image\n","Image\n","Image\n","KPIT Technology Ltd. : Data Analyst 16th Apr 2018  8th Feb 2019 Responsibilities:  Performed data analysis and visualization to support decision-making in automotive and manufacturing projects.  Extracted insights from large datasets, leading to improved efficiency in production processes and supply chain management. IL&FS Securities Ltd.: Developer May 2, 2017  Mar 30, 2018 Responsibilities:  Automated processes using Unix and developed shell scripts to streamline operations.  Implemented Python for file transfers and integrated\n"]}]},{"cell_type":"code","source":["# Function to generate an embedding for the text query\n","def generate_text_embedding(query):\n","    text_input = processor(text=query, return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n","    text_embedding = clip_model.get_text_features(**text_input)\n","    text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)  # Normalize embedding\n","    return text_embedding.detach().cpu().numpy()\n","\n","# Example text query asking for an image\n","text_query = \"Provide me an image of biodata.\"  # Example text query\n","text_query_embedding = generate_text_embedding(text_query)\n","\n","# Search the vector store using the text embedding\n","results = vector_store.similarity_search_by_vector(text_query_embedding[0], k=10)  # Retrieve top 10 matches\n","\n","print(\"\\nResults for Query About an Image:\")\n","for result in results:\n","    # Check if the result is an image based on metadata\n","    if result.metadata.get(\"type\") == \"image\":\n","        display_image(result.page_content)  # Replace with actual display logic for the image\n","    else:\n","        print(\"Description:\", result.page_content)  # Show text description for non-image results\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_9Kut1MYniF","executionInfo":{"status":"ok","timestamp":1729854900806,"user_tz":-330,"elapsed":487,"user":{"displayName":"Suryakanta Karan (M22AIE207)","userId":"09990899035878511614"}},"outputId":"5bd29410-3305-48aa-b9e5-85d2e614f9f1"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Results for Query About an Image:\n","Description: Sampling) and Top-K Sampling techniques in cloud environments to optimize text generation models, balancing response quality and computational efficiency\n","Description:  Trained machine learning models (e.g., Decision Trees, Random Forest, and Neural Networks) to classify fraudulent and legitimate transactions, achieving high precision and recall scores.  Employed advanced anomaly detection techniques to identify deviations in transaction patterns, flagging potential fraud cases for review.  Integrated real-time fraud detection mechanisms into banking platforms, ensuring the system could detect and prevent fraudulent transactions as they occur.  Tuned model hyperparameters and adjusted detection thresholds to reduce false positives and\n","Description: and Entity Recognition:  Developed predictive models using machine learning algorithms (e.g., XGBoost, LSTM) to forecast commodity prices, leading to a 15% improvement in decision-making.  Integrated real-time market data using APIs, enabling up-to-date trading insights and enhancing trading strategies.  Designed AI algorithms for risk management, reducing potential losses by analyzing market volatility.  Collaborated with traders and financial analysts to implement an automated trading system, improving trade execution speed and profitability.  Deployed models using\n","Description:  Lead and mentored data science teams to ensure the successful delivery of data science projects, providing guidance on best practices, and ensuring compliance with relevant data privacy and security regulations.  Leveraged Large Language Models (LLMs) for natural language processing (NLP) tasks, such as text preprocessing, feature extraction, Question answering, and sentiment analysis.  Developed End to end RAG LLM App Using Llama2, BART, GPT-3.5, LlamaIndex models with Huggingface and OpenAI.  Implemented data cleanup and\n","Description:  Hands-on experience with containerization technologies like Docker and Kubernetes  Familiar with CI/CD best practices, Git, Unit Testing, ML Ops and microservices for API integration, Pipeline and model deployment Professional Experience Wissen Technology: Senior Lead Data Scientist 16th Dec 2022  Present Responsibilities:  Collaborated with cross-functional teams to identify business requirements and technical specifications, leveraging Large Language Models (LLMs) for textual and audio data, ensuring accurate interpretation of stakeholder needs and alignment with technical solutions.\n","Description: Suryakanta Karan M: +91 8770228646 Email: suryakantakaran93@gmail.com LinkedIn: in/suryakanta-karan-595b4b34/ Professional Summary  Seasoned Sr. Lead Data Scientist with over 10+ years of expertise in developing, fine-tuning, and deploying Large Language Models (LLMs) such as GPT and LLaMA. Proven ability to deliver scalable NLP solutions using cutting-edge techniques including Retrieval-Augmented Generation (RAG), Reinforcement Learning from Human Feedback (RLHF), and advanced prompt engineering. Demonstrated leadership in managing cross-functional teams, mentoring engineers, and driving AI innovations that improve model accuracy\n","Description: machine unlearning techniques to ensure the removal of unnecessary or outdated information, improving the efficiency and relevance of the models.  Injected incremental data on a weekly basis from various sources such as Confluence, web APIs, xlsx, CSV files, and direct database connections into LLMs, enhancing the accuracy of in-house question-answering systems by 20% and improving user engagement with minimal intervention.  Designed deep learning models with TensorFlow, PyTorch, and Keras to classify unstructured data such as\n","Description: Feedback (RLHF), prompt engineering, and instruction tuning  Expertise in tokenization and embeddings (ELMo, GloVe, FastText)  Experience with Optical Character Recognition (OCR) and NLP frameworks for document processing  Utilized advanced fine-tuning techniques such as LoRA, QLoRa, federated learning, and machine unlearning for optimizing models AI Techniques  Expertise in federated learning, differential privacy, and privacy-preserving AI models  Deep learning models such as Transformers, LSTMs, GANs, VAEs, and Neural Networks  Real-time NLP streaming models\n","Description: handle structured and unstructured data from various documents. Deployed the solution across multiple business processes, improving operational speed and accuracy in document management. Education  Masters in Artificial Intelligence from Indian Institute of Technology  Jodhpur  2025  B. Tech (CSE) from Biju Patnaik University of Technology Odisha in 2014 Certifications & Courses  Machine Learning with Python Programming: iHUB DivyaSampark, IIT Roorkee & RBPL  Oracle Database 11g Administrator Certified Associate (OCA)\n","Description: (RAG) and Reinforcement Learning with Human Feedback (RLHF) to deliver accurate and context-aware responses.  Implemented data injections to incorporate real-time, domain-specific knowledge, enhancing the system's ability to answer queries with up-to-date and relevant information.  Utilized vector databases (e.g., Pinecone, FAISS, or Milvus) to store and retrieve high-dimensional embeddings, enabling efficient and scalable similarity search across large datasets.  Integrated pre-trained Large Language Models (LLMs) like GPT-3.5 and Llama2, fine-tuned with custom data for specialized knowledge\n"]}]}]}